
\chapter{Examining Model Behavior}
\label{chap:investigating}

In this chapter, we observe behaviors of neural \acp{lm} in specific \ac{d2t} generation scenarios, building upon custom datasets.

In \autoref{sec:rel2text}, we examine the capabilities of \acp{plm} to describe unseen relations between entities in knowledge graphs. For this problem, existing \ac{d2t} datasets are not able to discern memorization from generalization. We thus collect a custom dataset with a large variety of relation labels, including unseen labels in the test set. Using our dataset, we investigate whether the models can correctly describe the relations they have not seen in the training data. We find out that the models can generalize unseen labels as long as the labels are human-readable and unambiguous, which is often (but not always) fulfilled in real-world data.

In \autoref{sec:quintd}, we investigate the abilities of open \acp{llm} for \ac{d2t} generation from common formats such as JSON, CSV, and Markdown. To prevent data contamination, we scrape unlabeled data from public sources across five domains. Using \ac{llm}-based referenceless metric and human annotators, we quantify the semantic accuracy of the generated texts with respect to the input data. We find out that although the descriptions are fluent, most of them contain semantic errors.

\section{Describing Relations in Knowledge Graphs}
\label{sec:rel2text}
\begin{refbox}
    This section is based on the paper \emph{Mind the Labels: Describing Relations in Knowledge Graphs With Pretrained Models} \cite{kasnerMindLabelsDescribing2022}, joint work with Ioannis Konstas and Ondřej Dušek. The work was published in the Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2023). The project was led by the author of the thesis; Ioannis Konstas and Ondřej Dušek supervised the project.
\end{refbox}
We investigate how human-readable data labels help \acp{plm} with \ac{d2t} generation. We start by noticing that \acp{plm} can use labels such as column headings, keys, or relation names to generalize to out-of-domain examples. The question is (a) whether this ability is robust enough and (b) how accurate the outputs are in cases where these labels are ambiguous or incomplete. To answer this question, we focus on describing a relation between two entities.

For our experiments, we collect \textsc{Rel2Text}: a novel dataset for verbalizing a diverse set of 1,522 unique relations from three large-scale knowledge graphs (Wikidata, DBPedia, YAGO). We evaluate model outputs on unseen relations using a combination of automatic metrics and manual analysis. We find that although \acp{plm} for \ac{d2t} generation expectedly fail on unclear cases, models trained with a large variety of relation labels are surprisingly robust in verbalizing novel, unseen relations. We argue that using data with a diverse set of clear and meaningful labels is key to training \ac{d2t} generation systems capable of generalizing to novel domains. We release the code and data for our experiments on Github.\footnote{\url{https://github.com/kasnerz/rel2text}}


% 
% In this paper, 

\subsection{Motivation}
\ac{d2t} generation systems need to accurately capture the semantics of relations between values in the data. However, the data labels such as relation names \cite{farber2018linked,haller2022analysis}, table headings \cite{parikhToTToControlledTableToText2020}, or meaning representation keys \cite{dusekEvaluatingStateoftheartEndtoEnd2020} may provide only superficial or---if the labels are abbreviations, such as in the Rotowire dataset \cite{wiseman2017challenges}---no usable hints about the data semantics.


% \begin{figure}[t]

\begin{table}[t!] \small
    \centering
    \begin{tabular}{llp{3.7cm}l} \toprule
        \textbf{label}    & \textbf{property id}                                       & \textbf{verbalization}                                               & \textbf{note}                       \\ \midrule
        \textit{part of}  & \href{https://www.wikidata.org/wiki/Property:P361}{P361}   & \eh{} is part of \et{}.                                              & can be used verbatim                \\\cdashlinelr{1-4}
        \textit{duration} & \href{https://www.wikidata.org/wiki/Property:P2047}{P2047} & \eh{} lasted for \et{}.                                              & unambiguous verbalization           \\\cdashlinelr{1-4}
        \textit{platform} & \href{https://www.wikidata.org/wiki/Property:P400}{P400}   & \eh{} is available on \et{}.\newline\eh{} runs on \et{}.             & multiple equivalent lexical choices \\\cdashlinelr{1-4}
        \textit{occupant} & \href{https://www.wikidata.org/wiki/Property:P466}{P466}   & \et{} is occupied by \eh{}.\newline\eh{} plays at \et{}.             & semantics depends on entities       \\\cdashlinelr{1-4}
        % \textit{country}  & \href{https://www.wikidata.org/wiki/Property:P17}{P17}     & \eh{} was born in \et{}. \newline \eh{} is located in \et{}.         & semantics depends on entities       \\\cdashlinelr{1-4}
        \textit{parent}   & \href{https://www.wikidata.org/wiki/Property:P8810}{P8810} & \eh{} is the parent of \et{}. \newline \et{} is the parent of \eh{}. & ambiguous relation direction        \\\bottomrule
    \end{tabular}
    \captionof{table}{Example relation labels and the variability in their verbalizations. \eh{} and \et{} denote subject and object in the triple, respectively. The Wikidata page for each relation is available at \url{https://www.wikidata.org/wiki/Property:<property_id>}.}
    \label{tab:rel2text:example}
\end{table}

\acp{plm} such as BART \cite{lewisBARTDenoisingSequencetoSequence2019} or T5 \cite{raffelExploringLimitsTransfer2019} can quickly adapt to new domains and exhibit robustness to out-of-domain inputs. We investigate to what extent are \acp{plm} limited by the expressivity of the data labels. A suitable testing ground is the task of describing individual \acs{rdf}\glsunset{rdf} triples in a \ac{kg}, as shown in \autoref{tab:rel2text:example}. In this task, there is a wide range of lexical choices for the relation label, while the entities can be copied verbatim or with only minor morphological changes. To illustrate the problem, consider the last example in \autoref{tab:rel2text:example}: the model can use its representation of \emph{``parent''} to understand there is a \emph{``is-a-parent-of''} relation between the entities, but it has to infer (or guess) who is the parent of whom. Even in less ambiguous cases, the model still has to correctly capture the intended semantics of the relation (e.g. \emph{``occupant''} meaning \emph{``home team''}).


Current human-annotated datasets for \ac{d2t} generation are not suitable for investigating this problem, as they contain only a small number of relations and rarely contain any unseen relations in the test set \cite{mille2021automatic}. The only existing datasets covering verbalizations of a wider range of \ac{kg} relations are based on \emph{model-generated outputs} \cite{agarwalKnowledgeGraphBased2021,amaral2022wdv}. For this reason, we collect a novel human-annotated dataset for the task.

Our aim is also to investigate whether incorporating long-form \emph{descriptions} of data labels helps improve model outputs. Previous works have reached contradictory conclusions: \citet{wang2021kepler} use descriptions of relations instead of their labels for relation embeddings, concluding that it results in worse performance on downstream tasks. Conversely, \citet{kale-rastogi-2020-template} and \citet{lee2021dialogue} improve the performance of their systems by including schema descriptions on the input for the dialogue state tracking and dialogue response generation systems.

Lastly, we investigate \emph{verbalizing single triples} as a stand-alone task. As we have shown (\Cref{sec:iterative,sec:pipeline,sec:sem-acc}), in line with other works \cite{xiangASDOTAnyShotDatatoText2022,kale-rastogi-2020-template,gupta2020infotabs,neeraja2021incorporating}, transforming triples to text helps for \acp{plm}-based data processing. The works above use various methods for converting individual triples to text, ranging from simple templates and rule-based systems to prompting \acp{plm}; however, none of them investigate how to apply these approaches to novel relations.


% 

% Using the \textsc{Rel2Text} dataset, we evalute the ability of \acp{plm} to verbalize relations which were not present in the training set. We consider both models finetuned on other relations in our dataset and models finetuned on datasets from a related domain. We also experiment with scenarios involving few-shot finetuning, training on masked labels, and extending the labels with descriptions (§\ref{sec:analysis},~\ref{sec:results}).

% We find that the \acp{plm} are quite robust in verbalizing a diverse set of relations based on their label (achieving \textasciitilde 90\% of overall entailment probability). We show that semantically unfaithful model outputs are often caused by incomplete, ambiguous, or noisy input data.



% Somewhat suprisingly, we also show that longer relation descriptions do not provide substantial improvements over using short labels.  

% However, even for data using short relation labels, the model trained on verbalizing relations can achieve results comparable to verbalizing relations using manual templates in two downstream tasks (§\ref{sec:downstream}).



\subsection{\textsc{Rel2Text} dataset}
\label{sec:rel2text:data}
For our experiments, we need data with diverse labels and their human verbalizations. We start by collecting a large set of relations from three large-scale \acp{kg} (Wikidata, DBPedia, and YAGO). For each relation, we collect its label, textual description, and up to five triples in which the relation occurs in the \ac{kg}. We then use human annotators to collect a \emph{verbalization} for each triple, i.e., a short sentence capturing the meaning of the triple. After filtering, our dataset---which we call \textsc{Rel2Text} (\underline{R}e-writing \underline{e}dge \underline{l}abels to Text)\footnote{Or simply ``Relations-to-Text''.}---contains 4,097 single triples covering 1,522 unique relations. We describe the data collection process in the following paragraphs.


\paragraph{Input Data}
An \acs{rdf} triple is a tuple $t = (s, r, o)$, where $r$ denotes the relation\footnote{In previous sections, we have also called this constituent a \emph{predicate}; these notions are equivalent.} between the subject $s$ and the object $o$.
We retrieve triples from three open large-scale \acp{kg} encoding factual knowledge:

\begin{itemize}
    \item \textbf{Wikidata} \cite{vrandevcic2014wikidata} is a large-scale Wikipedia-based \ac{kg} created using collaborative editing. With approximately 10,000 human-created relations equipped with descriptions\footnote{\url{https://www.wikidata.org/wiki/Wikidata:Database_reports/List_of_properties/all}}, it is by far the largest source of variety in relation labels.
    \item \textbf{YAGO} \cite{pellissier2020yago} is a \ac{kg} which builds upon factual knowledge from Wikidata, but uses a limited set of 116 pre-defined relations from \texttt{schema.org} \cite{guha2016schema} mapped to a subset of Wikidata relations.
    \item \textbf{DBPedia} \cite{auer2007dbpedia,lehmann2015dbpedia} is a \ac{kg} that maps Wikipedia infotables to a predefined ontology containing 1,355 relations, about 350 of which are accompanied by a description.
\end{itemize}

We query all \acp{kg} using their openly available endpoints to retrieve a list of relations in each \ac{kg}. For each relation, we retrieve up to five \textit{triples} that use this relation and the relation \textit{description}, i.e., a short explanatory text.
If present, we also retrieve descriptions for the subject and the object. We apply a set of filtering heuristics, leaving out, e.g., relations describing \ac{kg} metadata or identification numbers.\footnote{Relations describing various IDs make up a large portion of relations in Wikidata. Since we focus on diversity instead of coverage, we decided not to include these relations in our dataset.} In this way, we collect 7,334 triples with 1,716 relations in total.

\paragraph{Annotation Process}
We collect human-written verbalizations for all input triples using Prolific.\footnote{\url{https://www.prolific.co/}} We built a web interface where human annotators are shown a single triple $t$ and asked to describe it in a single sentence. The annotators are encouraged to re-use the entities in their original form, but they can change the form if necessary. The annotators can also report noisy inputs. We employed 420 annotators in total, each of which annotated 20 examples. We set the average reward per hour according to the platform recommendations to  \textsterling{}7.29 per hour, and we accepted all the inputs that passed our built-in checks.

\paragraph{Postprocessing the Data}
A considerable portion of the collected verbalizations contain typos and grammatical errors, misunderstood meaning of the relation, or extra information in the input. To ensure the high quality of our data, we manually examined all crowdsourced examples and annotated them as \textit{OK}, \textit{noisy}, \textit{corrupted}, or \textit{containing extra information}. For our experiments, we only use the subset of our dataset with \textit{OK} annotations, one per input triple (4,097 examples, 1,522 distinct relations).


\subsection{Analysis and Experiments}
\label{sec:rel2text:analysis}
In our analysis, we are interested in the following research questions:
\begin{itemize}
    \item \textbf{RQ1:} Are the PLMs finetuned for \ac{d2t} generation able to describe relations \textit{not present in the finetuning corpus}?
    \item \textbf{RQ2:} How many \textit{training examples} do the PLMs need to generate satisfactory outputs?
    \item \textbf{RQ3:} How do the PLMs behave when provided \textit{limited lexical cues} about the relation?
    \item \textbf{RQ4:} Can relation \textit{descriptions} help to clarify ambiguous cases and improve the semantic accuracy of the outputs?
\end{itemize}

\paragraph*{Datasets}  We experiment with the following datasets, all of which focus on verbalizing factual information from \acp{kg} and use the same triple-based input data format:
\begin{itemize}
    \item \textsc{Rel2Text}: Our dataset with single triples from three \acp{kg} with 4,097 examples, 1,522 relations, and \textit{human-annotated} outputs.
    \item WebNLG \cite{ferreira20202020,gardentWebNLGChallengeGenerating2017}: A DBPedia-based triple-to-text dataset with 38k examples, 411 relations, up to 7 triples per example, and \textit{human-annotated} outputs. We use the English part of version 3.0 from HuggingFace.\footnote{\url{https://huggingface.co/datasets/web_nlg}}
    \item KeLM \cite{agarwalKnowledgeGraphBased2021}: A Wikidata-based dataset with 11M examples, 1,519 relations, up to 13 triples per example, and \textit{model-generated} outputs. We use the dataset released by the authors, splitting it into a 1:100 ratio for validation and training data.
\end{itemize}
To answer the research questions, we divide our \textsc{Rel2text} dataset into a training and test split. We then use the \textsc{Rel2Text} \emph{test set} to evaluate a finetuned BART model \cite{lewisBARTDenoisingSequencetoSequence2019}. To answer \emph{RQ1}, we compare the performance of BART finetuned on the \textsc{Rel2Text} training set with BART finetuned on WebNLG and KeLM. Using \textsc{Rel2text} only, we then prepare various setups for answering \emph{RQ2}, \emph{RQ3}, and \emph{RQ4}.


\paragraph{Rel2Text Data Split} We use approximately 15\% of the \textsc{Rel2Text} examples for the \emph{test set}. To ensure maximum fairness and focus on model generalization to unseen relations, we do not include in the \textsc{Rel2Text} test set any relations that have an exact string match with a relation in KeLM, WebNLG, or the \textsc{Rel2Text} training set. We also exclude any relations for which the maximum semantic similarity\footnote{Computed as cosine similarity between embeddings of the labels, which are encoded using \texttt{all-distilroberta-v1} from SBERT \cite{reimers-gurevych-2019-sentence}.} to any KeLM/WebNLG/\textsc{Rel2Text} training relation exceeds a threshold of $0.9$. We set this threshold empirically to exclude relations that are almost synonymous, but slightly lexically different.
We use 90\% of the remaining examples for the training set and 10\% for the validation set.

\paragraph{Experimental Setup} We split the camel case in the relation labels. For finetuning the models, we linearize the input triples by marking the triple constituents with special tokens that we add to the model vocabulary. In a default scenario, we finetune BART-base \cite{lewisBARTDenoisingSequencetoSequence2019} for 10 epochs and select the best checkpoint using validation \acs{bleu} score, then use greedy decoding to produce outputs. We repeat each experiment with five random seeds, averaging the results.


\paragraph{Compared Systems} We compare the following systems:
\begin{itemize}
    \item \textbf{Copy Baseline}: We introduce a simple \textit{copy} baseline by outputting the triple constituents separated by space: ``$s\text{ }r\text{ }o$''.



    \item \textbf{Full Training Data}: We use the default setup on full \textsc{Rel2Text} and WebNLG training sets. For KeLM (which is about 300$\times$ larger than WebNLG), we finetune the model only for one epoch. We denote the trained models \BARTr{}, \BARTw{}, and \BARTk{}, respectively.

    \item \textbf{Limited Training Data}: For the limited training data setup, we prepare few-shot splits from \textsc{Rel2Text} as subsets containing $N=$ \{25, 50, 100, 200\} relations with a single example per relation. We select examples at random, ensuring that each few-shot split is a subset of the larger splits. We finetune the \textit{fewshot-N} models for 10 epochs without validation, using the last checkpoint.

    \item \textbf{Limited Lexical Cues}: We investigate how the models behave if we do not include the relation labels at all. We consider three scenarios:
          \begin{itemize}
              \item \textit{mask-test} -- We train the model on \textsc{Rel2Text} in the standard training setup. For testing, we replace the relation labels in  \textsc{Rel2Text} with the \textit{<mask>} token.
              \item \textit{mask-train} -- For training, we replace the relation labels in  \textsc{Rel2Text} with the \textit{<mask>} token. We test the model on \textsc{Rel2Text} in the standard evaluation setup.
              \item \textit{mask-all} -- We replace the relation labels in  \textsc{Rel2Text} with the \textit{<mask>} token for both training and testing.
          \end{itemize}


    \item \textbf{Incorporating Descriptions}: Our dataset contains short textual descriptions of the relations, which may be useful to disambiguate its meaning and provide additional cues to the model. We consider two scenarios:
          \begin{itemize}
              \item \textit{desc-repl} -- We replace the relation label with its description.
              \item \textit{desc-cat} -- We concatenate the relation description with the input, separated using the special token \textit{<rel\_desc>}.
          \end{itemize}
\end{itemize}

\subsection{Evaluation Setup}
\paragraph{Automatic Metrics}


We evaluate generated outputs using an extensive set of automatic metrics from the GEM-metrics\footnote{\url{https://github.com/GEM-benchmark/GEM-metrics}} package \cite{gehrmannGEMBenchmarkNatural2021}. We use \acs{bleu} \cite{papineni2002bleu}, \mbox{METEOR} \cite{banerjee-lavie-2005-meteor}, and BLEURT \cite{sellam2020bleurt} for measuring lexical similarity. For semantic similarity, we use NUBIA \cite{kaneNUBIANeUralBased2020} along with its individual features: the semantic similarity score (SS) on a 0-5 scale, the contradiction (C), neutral (N), and entailment (E) probabilities, and the perplexity score (PPL). To assess lexical diversity, we measure the number of unique n-grams (U-1), conditional entropy of bi-grams (CE2), and the mean segmental type-token ratio over segment lengths of 100 (MSTTR). We also measure the average output length in tokens (len). See \autoref{sec:evaluation} for a detailed description of the metrics.



\paragraph{Manual Error Analysis} Based on our preliminary observations, we identify four sources of model errors: semantic errors (\textsc{Sem}), with a swap of the relation direction (\textsc{Dir}) as a special case, too literal (\textsc{Lit}), i.e., containing awkward or misleading phrasing, and grammar/lexical errors (\textsc{Lex}). We further identified two types of input data errors: ambiguous relations (\textsc{Ent}) and relations with unclear labels (\textsc{Lbl}). Examples are shown in \autoref{tab:rel2text:cat}.
For error analysis, we select 100 random examples together with their corresponding outputs from the \textit{\BARTr}, \textit{\BARTw}, \textit{\BARTk}, \textit{fewshot-100}, \textit{mask-all} and \textit{desc-cat} models. Without revealing the output sources, we mark all error categories that apply.
\begin{table*}[t]
    \centering\footnotesize
    % \setlength{\tabcolsep}{4pt}
    % \renewcommand{\arraystretch}{1.25}
    \begin{tabular}{p{0.5cm}p{1cm}p{11.6cm}} \toprule

         & \textbf{Label} & \textbf{Example inputs and outputs (\red{\xmark} incorrect, \green{\cmark} correct)}                                                                                                                                                                                    \\ \midrule
        %
        \multirow{12}{*}{\rotatebox[origin=c]{90}{\textit{model}}}
        % 
         & \textsc{Sem}   & \emph{(Yousra Matine, sport country, Morocco)} \newline \red{\xmark} Yousra Matine was born in Morocco. [\emph{mask-mask}] \newline  \green{\cmark} Yousra Matine plays for Morocco. [\BARTr]                                                                           \\[2mm]
         & \textsc{Dir}   & \emph{(Kentucky Channel, former broadcast network, KET ED)} \newline \red{\xmark} KET ED was broadcast on Kentucky Channel ED. [\emph{fewshot-100}] \newline  \green{\cmark} The Kentucky Channel was broadcast on KET ED. [\BARTr]                                     \\[2mm]
         & \textsc{Lit}   & \emph{(Vietnam Television, first air date, 1970-09-07)} \newline \red{\xmark} The first air date of Vietnam Television was 1970-09-07. [\BARTk] \newline  \green{\cmark} Vietnam Television first aired on 1970-09-07. [\BARTr]                                         \\[2mm]
         & \textsc{Lex}   & \emph{(RPG-43, used in war , The Troubles)} \newline \red{\xmark} RPG-43 was used in the The Troubles. [\BARTr] \newline  \green{\cmark} The RPG-43 was used in the Troubles. [\BARTk]                                                                                  \\[1mm]\cdashlinelr{1-3}\\[-3mm]
        %
        \multirow{6}{*}{\rotatebox[origin=c]{90}{\textit{data}}}
        %
         & \textsc{Ent}   & \emph{(The Age of Entitlement, by artist, The Basics)} \newline \red{\xmark} The Age of Entitlement was written by The Basics. [\BARTk] \newline  \green{\cmark} The Age of Entitlement was recorded by The Basics.  [\BARTr]                                           \\[2mm]
         & \textsc{Lbl}   & \emph{(General Motors Epsilon platform, vehicle, Cadillac XTS)} \newline \red{\xmark} General Motors Epsilon is a vehicle similar to the Cadillac XTS. [\BARTw] \newline  \green{\cmark} General Motors Epsilon platform is used in the Cadillac XTS. [\emph{desc-cat}] \\
        \bottomrule
    \end{tabular}
    \caption{Error categories used in manual analysis, with examples of errors found and corresponding correct verbalizations (square brackets denote the model).
        Model error types (top):
        \textsc{Sem} -- The output is semantically incorrect,
        \textsc{Dir} -- The direction of the relation is swapped,
        \textsc{Lit} -- The verbalization is too literal,
        \textsc{Lex} -- There is a lexical error in the output.
        %
        Input data error types (bottom):
        \textsc{Ent} -- The verbalization may depend on the entities,
        \textsc{Lbl} -- The relation label is not clear.
    }
    \label{tab:rel2text:cat}
\end{table*}


\subsection{Findings from Automatic Metrics}
\label{sec:rel2text:results}

\begin{table*}[t]\centering
    \footnotesize
    \setlength{\tabcolsep}{5pt}
    \begin{tabular}{lc>{\hspace{-2mm}}c>{\hspace{-2mm}}cc>{\hspace{-2mm}}c>{\hspace{-2mm}}c>{\hspace{-2mm}}c>{\hspace{-2mm}}cc>{\hspace{-2mm}}c>{\hspace{-2mm}}c>{\hspace{-2mm}}c>{\hspace{-2mm}}c>{\hspace{-2mm}}c}\toprule
        \multirow{2}{*}{} & \multicolumn{3}{c}{\textbf{Lexical}} & \multicolumn{5}{c}{\textbf{Semantics}} & \multicolumn{5}{c}{\textbf{Referenceless}}                                                                                              \\\cmidrule(r){2-4}\cmidrule(r){5-9}\cmidrule{10-14}
                          & \bf \acs{bleu}                       & \bf MET                                & \bf BLR                                    & \bf SS & \bf C & \bf N & \bf E & \bf NB & \bf U-1 & \bf CE-2 & \bf TTR & \bf PPL & \bf len \\\midrule
        \it human         & -                                    & -                                      & -                                          & -      & -     & -     & -     & -      & 1785    & 2.13     & 0.62    & 5.88    & 9.55    \\
        \it copy          & 29.04                                & 37.52                                  & 0.09                                       & 4.79   & 1.22  & 7.57  & 91.21 & 0.74   & 1606    & 1.17     & 0.7     & 7.55    & 6.72    \\\cdashlinelr{1-14}
        \it \BARTr{}      & 52.54                                & 44.86                                  & 0.54                                       & 4.72   & 3.50  & 4.65  & 91.85 & 0.88   & 1661    & 1.96     & 0.58    & 5.89    & 9.16    \\
        \it \BARTw{}      & 41.99                                & 41.59                                  & 0.41                                       & 4.65   & 3.68  & 6.93  & 89.39 & 0.86   & 1651    & 2.54     & 0.56    & 5.65    & 10.29   \\
        \it \BARTk{}      & 46.74                                & 42.94                                  & 0.46                                       & 4.70   & 3.95  & 5.29  & 90.77 & 0.86   & 1652    & 2.32     & 0.56    & 5.83    & 9.71    \\\cdashlinelr{1-14}
        \it fewshot-25    & 31.13                                & 35.52                                  & -0.02                                      & 3.94   & 8.35  & 27.26 & 64.39 & 0.65   & 1445    & 2.93     & 0.52    & 5.34    & 10.67   \\
        \it fewshot-50    & 40.60                                & 40.05                                  & 0.25                                       & 4.44   & 8.04  & 13.12 & 78.84 & 0.76   & 1536    & 2.31     & 0.55    & 5.79    & 9.90    \\
        \it fewshot-100   & 45.88                                & 42.38                                  & 0.38                                       & 4.53   & 6.34  & 10.60 & 83.06 & 0.81   & 1600    & 2.13     & 0.57    & 5.85    & 9.57    \\
        \it fewshot-200   & 48.67                                & 43.34                                  & 0.44                                       & 4.58   & 5.40  & 9.03  & 85.57 & 0.83   & 1626    & 2.04     & 0.58    & 5.89    & 9.36    \\\cdashlinelr{1-14}
        \it mask-test     & 42.45                                & 38.52                                  & 0.25                                       & 3.99   & 14.91 & 18.47 & 66.62 & 0.65   & 1669    & 1.96     & 0.61    & 5.69    & 8.96    \\
        \it mask-train    & 46.90                                & 43.15                                  & 0.43                                       & 4.55   & 5.85  & 11.55 & 82.61 & 0.81   & 1646    & 2.00     & 0.57    & 5.91    & 9.74    \\
        \it mask-all      & 42.53                                & 38.49                                  & 0.24                                       & 3.85   & 17.58 & 25.15 & 57.26 & 0.61   & 1677    & 1.96     & 0.61    & 5.66    & 9.16    \\\cdashlinelr{1-14}
        \it desc-repl     & 49.35                                & 42.85                                  & 0.47                                       & 4.57   & 5.78  & 8.80  & 85.42 & 0.82   & 1693    & 1.94     & 0.59    & 5.86    & 9.18    \\
        \it desc-cat      & 53.07                                & 45.04                                  & 0.55                                       & 4.72   & 3.46  & 4.66  & 91.88 & 0.87   & 1668    & 1.91     & 0.59    & 5.92    & 9.11    \\
        \bottomrule
    \end{tabular}
    \caption{The summary of evaluation using automatic metrics on \textsc{Rel2text} test set. \textbf{MET} = METEOR, \textbf{BLR} = BLEURT, \textbf{TTR} = MSTTR. See \autoref{sec:rel2text:analysis} for the descriptions of the models and metrics.}
    \label{tab:rel2text:auto}
\end{table*}
\autoref{tab:rel2text:auto} shows automatic scores for all our models.

\paragraph{Lexical and Semantic Similarity}
\BARTr{} is the best among the fully trained models regarding lexical overlap metrics, which is expected, as it is trained on the most similar reference distribution. \BARTw{} and \BARTk{} models are almost equal in terms of semantic consistency, achieving around 90\% average entailment probability, which is on par with the copy baseline.

\paragraph{Few-shot Models} For the few-shot models, semantic similarity is much lower (e.g., the average entailment probability is between 65\% and 85\%), showing that there is a certain minimum amount of data needed to achieve consistent outputs. As we show in \autoref{fig:rel2text:fewshot}, using more examples to train the model generally helps decrease variance and increase performance across various metrics.

\paragraph{Masked Relations} Interestingly, the models which do not see the relations during test time (\textit{mask-test} and \textit{mask-all}) still achieve around 60\% average entailment probability, similarly to the worst few-shot model. Although their rate of contradictions is higher than for other models, the results suggest that in many cases, the guessed relation is semantically consisten with the correct relation. Another interesting observation is that the \textit{mask-train} model (trained not to use the labels) can use the labels provided at test time to improve the outputs considerably (contradiction rate drops from 17\% to 5\% compared to \textit{mask-all}).

\paragraph{Influence of Descriptions} The fact that the short labels are both sufficient and necessary for successful verbalization is emphasized by the fact that the \textit{desc-repl} model is worse than \BARTr{} (although the descriptions are longer and supposedly explain the relation semantics). Moreover, the benefits of concatenating the descriptions alongside the relation labels (\textit{desc-cat}) are negligible, only slightly improving lexical similarity metrics (0.5 \acs{bleu} point gain over \BARTr{}).

\paragraph{Lexical Diversity} In terms of lexical diversity, human references use more unique n-grams, but the model outputs are very similar in other aspects. It remains to be seen if the model outputs can stay semantically consistent with diversity-focused decoding techniques such as nucleus sampling \cite{holtzman2019curious}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{img/rel2text-fewshot.pdf}
    \caption{Boxplots for selected metrics from \autoref{tab:rel2text:auto} w.r.t. the number of examples (displayed on the \textit{x}-axis, $\textit{full} = 1522$), taking into account variance from individual random seeds.}\label{fig:rel2text:fewshot}
\end{figure}

\subsection{Findings from Manual Error Analysis}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\textwidth]{img/rel2text-graph.pdf}
    \caption{Number of annotated errors per model (see \autoref{tab:rel2text:cat} for the description of error categories and \autoref{sec:rel2text:analysis} for the models). The striped part signifies that the label of the input was marked as unclear.}
    \label{fig:rel2text:manual}
\end{figure}
Results are summarized in \autoref{fig:rel2text:manual}; examples of model outputs for each error type are shown in \autoref{tab:rel2text:cat}.

\paragraph{Naturalness of Expressions} The \BARTk{} and \BARTw{} models use expressions that are too literal (\textsc{Lit}) in 23 and 29 cases, respectively, while the \BARTr{} and \textit{desc-cat} models do the same only in 11 cases (5 out of which are marked as \textsc{Lbl}, i.e., with an unclear label). This suggests that the variability of our dataset helps models to apply more natural expressions, especially if the relation is understandable from its label.

\paragraph{Semantic Errors} There is a near-constant portion of examples where the models make a semantic error (\textsc{Sem}) \textit{and} the input is marked as needing an extra description (\textsc{Lbl}). The models also make relatively many semantic errors, most prominently in the case of the \textit{fewshot-100} and the \textit{mask-all} models. The \textit{mask-all} model made a semantic error in 78 cases, suggesting that guessing the exact relation just from the entities is difficult (although still possible in 22 cases). Moreover, the outcomes from this model are fluent (only 4 \textsc{Lex} errors), making it hard to detect faulty cases.
The case of swapping the relation direction (\textsc{Dir}) is surprisingly not that common, which is probably down to having only a few examples in our dataset prone to this kind of error.

\paragraph{Additional Clues} There were only 12 out of 100 examples annotated as \textsc{Ent}, which suggests that the verbalization of the relation can be mostly decided irrespective of the entities in the triple. Notably, the results for \BARTr{} and \textit{desc-cat} are very similar, rendering the impact of extra descriptions negligible.


\subsection{Applications to Downstream Tasks}
\label{sec:rel2text:downstream}
Given that the \BARTr{} model can describe relations from their labels with high accuracy, we investigate if we can use the model to replace manually created templates in downstream tasks. We select two qualitatively different tasks, both using the idea of transforming individual input triples to simple sentences as a preprocessing step: tabular reasoning and zero-shot data-to-text generation.


\begin{table}[t]\centering
    \small
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{lcccc}\toprule
        \textbf{premise repr.}              & \textbf{dev} & $\alpha_1$ & $\alpha_2$ & $\alpha_3$ \\\midrule
        OPR \cite{gupta2020infotabs}        & 76.78        & 75.30      & 68.46      & 64.63      \\
        BPR \cite{neeraja2021incorporating} & 77.04        & 74.44      & 67.46      & 63.17      \\
        \BARTr{} (ours)                     & 74.44        & 74.31      & 64.59      & 63.46      \\
        \bottomrule
    \end{tabular}
    \caption{Accuracy for the dev set and test sets  $\alpha_{1,2,3}$ from the \textsc{InfoTabS} dataset. The results are averaged over 3 random seeds.}
    \label{tab:rel2text:nli}
\end{table}


\paragraph{Tabular Reasoning} For the task of \ac{nli} from a table, \citet{gupta2020infotabs} represent each table cell using a simple template \text{``The \textit{key} of \textit{title} are \textit{value}.''}. \citet{neeraja2021incorporating} extend their approach by preparing a fine-grained set of rules\footnote{Formalized using more than 250 lines of Python code: \url{https://github.com/utahnlp/knowledge\_infotabs/blob/main/scripts/preprocess/bpr.py\#L120}} for individual entity categories. We replicate the setup of \citet{neeraja2021incorporating} for the original (OPR) and better (BPR) paragraph representation using their public codebase. We then replace their templates with our \BARTr{} model, verbalizing the triple (\textit{title}, \textit{key}, \textit{value}). The results are summarized in Table \ref{tab:rel2text:nli}. Our manual evaluation suggests that the sentences from our model are indeed more grammatical (even compared to BPR), but we observe comparable performance across all three test sets. In line with \citet{mccoy2019right}, we conclude that for classification tasks such as NLI, the input content appears to be more important than the input form.


\begin{table}[t]\centering
    \small
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{clcccc}\toprule
        \textbf{dataset}                   & \textbf{model} & \textbf{BLEU} & \textbf{METEOR} & \textbf{O} & \textbf{H} \\\midrule
        \multirow{2}{*}{\textit{filtered}} & orig           & 43.19         & 39.13           & 0.152      & 0.073      \\
                                           & \BARTr{}       & 45.39         & 38.97           & 0.056      & 0.161      \\\cdashlinelr{1-6}
        \multirow{2}{*}{\textit{full}}     & orig           & 42.92         & 39.07           & 0.051      & 0.148      \\
                                           & \BARTr{}       & 44.63         & 38.93           & 0.058      & 0.166      \\
        \bottomrule
    \end{tabular}
    \caption{Lexical similarity metrics (BLEU, METEOR) and ommission (O) and hallucinaton (H) rate; following the setup in \citet{kasner2022neural}.}\label{tab:rel2text:zeroshot}
\end{table}

\paragraph{Zero-shot Data-to-Text Generation} In \autoref{sec:pipeline}, we described our approach for zero-shot \ac{d2t} generation which requires transforming individual triples into text \cite{kasner2022neural}. Here, we replicate the setup on the WebNLG dataset, applying the \BARTr{} model instead of the templates. The results are summarized in Table \ref{tab:rel2text:zeroshot}.  We note that the pipeline using our model for preprocessing is able to achieve improvements of $\sim$2 \acs{bleu} points, at the cost of a slightly higher omission and hallucination rate, but crucially without needing the manual effort to create templates. A cursory examination shows that sentences produced by our model are qualitatively similar to the manual templates but more varied. Unlike the templates, our model may verbalize a relation differently depending on the context.
Overall, we argue that training a PLM on verbalizing individual relations can potentially replace the manual effort of creating simple templates, which will have a notable impact on scaling similar approaches to larger datasets.


\subsection{Discussion}

\paragraph{Takeaways} We showed that \acp{plm} can verbalize novel relations as long as the relation label is human-readable and unambiguous. However, when the cues about the relation are limited, the model will resolve to guessing. A takeaway for datasets that do not follow standard naming conventions, such as the Rotowire dataset \cite{wiseman2017challenges}, which uses abbreviations for column headers (e.g., FG3A stands for \textit{``the number of shots the player attempted beyond the arc''}), is that rephrasing the labels to natural language may increase the quality of outputs from neural systems. It is still an open question how to handle input data with noisy labels. We suggest detecting and fixing these cases prior to generation, for example, with a human-in-the-loop setup.


\paragraph{Implications for Large Language Models} We focused on \acp{plm}, which require at least several hundred examples to produce satisfactory results. With in-context learning, \acp{llm} may bring down the number of examples required close to zero. In this case, the models cannot learn the correct verbalizations from the training data, which makes using clear and unambiguous labels even more important. A follow-up work has shown that prompting \acp{llm} can bring comparable results to using a finetuned \ac{plm} in our scenario but requires a more complex setup for controlling the model outputs \cite{vejvarASPIROAnyshotStructured2023}.

\paragraph{Efficient Use of Relation Descriptions} To achieve more notable improvements with long-form descriptions of relations, it may be necessary to include a more detailed specification regarding the relation direction, type, or acceptable values. The model also needs to be able to reason about this specification, which could be achieved with the help of \acp{llm} and chain-of-thought reasoning \cite{weiChainThoughtPrompting2022,zhaoInvestigatingTabletoTextGeneration2023}.


\paragraph{Limitations} Our analysis is limited to verbalizing single triples, which is only a trivial case of graph-to-text generation. Nevertheless, we believe that this simplified setting allows us to distill insights that are still applicable to graph-to-text generation in general. We also focus only on the English part of the \acp{kg}: for more morphologically rich languages, an extra effort must be put into correctly inflecting the entities in the generated text.






\section{Data-to-Text Generation with Large Language Models}
\label{sec:quintd}

\begin{refbox}
    This section is based on the paper \emph{Beyond Traditional Benchmarks: Analyzing Behaviors of Open LLMs on Data-to-Text Generation} \cite{kasnerReferenceBasedMetricsAnalyzing2024}, joint work with Ondřej Dušek. The work is accepted to the Proceedings of the 62th Annual Meeting of the Association for Computational Linguistics (ACL 2024).
\end{refbox}
We investigate the specifics of \ac{d2t} generation with \aclp{llm} (\acsp{llm}\glsunset{llm}). To avoid benchmarking the models on data seen during pretraining, we do not use standard \ac{d2t} datasets described in \autoref{sec:datasets}. Instead, we capitalize on the ability of \acp{llm} to process structured data in standard formats such as JSON, CSV, and Markdown without task-specific finetuning. To collect the data, we design a tool for downloading structured data records from public APIs.
% We prepare the data for five \ac{d2t} generation tasks: generating weather forecasts, product descriptions, game summaries, chart captions, and entity descriptions. 
Using reference-free evaluation methods based on human annotators and GPT-4, we evaluate semantic accuracy of model outputs on the word level.
% Furthermore, we put open \acp{llm} in the locus of our investigation, promoting open and reproducible research. 
We find that although the outputs from the \acp{llm} are fluent, the semantic accuracy of the outputs is a major issue: both according to human annotators and GPT-4, more than 80\% of the outputs of open \acp{llm} contain at least one semantic error. We publicly release the code, data, and model outputs.\footnote{\url{https://d2t-llm.github.io/}}


\begin{table*}[ht]
    \small
    \centering
    \begin{tabular}{@{}llllll@{}} \toprule                                                                                                                  \\
        \textbf{Task Id}     & \textbf{Domain} & \textbf{Output type}      & \textbf{Source}                                   & \textbf{Format} \\ \midrule
        \texttt{openweather} & Weather         & five-day weather forecast & \href{https://openweathermap.org}{OpenWeather}    & JSON            \\
        \texttt{gsmarena}    & Technology      & product description       & \href{https://www.gsmarena.com}{GSMArena}         & JSON            \\
        \texttt{ice\_hockey} & Sport           & ice hockey game summary   & \href{https://rapidapi.com}{RapidAPI}             & JSON            \\
        \texttt{owid}        & Health          & chart caption             & \href{https://ourworldindata.org}{OurWorldInData} & CSV             \\
        \texttt{wikidata}    & World facts     & entity description        & \href{https://wikidata.org}{Wikidata}             & Markdown        \\\bottomrule
    \end{tabular}
    \caption{The domains and tasks included in the \datatool{} data collection tool we use for testing \ac{d2t} generation with \acp{llm}. In our experiments, we download 100 development and 100 test examples of input data for each domain.}
    \label{tab:quintd:data}
\end{table*}

\subsection{Motivation}
At the time of writing, the applicability of \acp{llm} \cite{ouyang2022TrainingLM,touvronLlamaOpenFoundation2023,jiangMistral7B2023,tunstallZephyrDirectDistillation2023} to \ac{d2t} generation remains underexplored. The only existing works evaluated proprietary models on a handful of well-established benchmarks \cite{axelssonUsingLargeLanguage2023,yuanEvaluatingGenerativeModels2023}.

However, the current \ac{d2t} generation benchmarks are not only getting saturated \cite{van_miltenburg_barriers_2023}, but also promote optimization towards traditional reference-based evaluation metrics, which were shown to correlate poorly with human judgment \cite{gehrmannRepairingCrackedFoundation2022,vanderleeHumanEvaluationAutomatically2021,novikovaWhyWeNeed2017}. When it comes to the models, using closed \acp{llm} \cite{openai2023gpt4,chatgpt} is increasingly considered a bad research practice due to its non-reproducibility \cite{rogers2023closed,chen2023chatgpt}, and contamination of LLM training data with standard benchmarks further restricts the space for experiments \cite{golchin2023time,aiyappa-etal-2023-trust,balloccu2024leak}.

Our analysis circumvents these issues by (1) collecting ad-hoc structured data without human-written references, (2) using reference-free evaluation methods for annotating errors on the level of individual tokens, and (3) focusing on open \acp{llm}, which -- apart from being more accessible -- also achieve increasingly better results across tasks \cite{zheng2023judging,open-llm-leaderboard}.

% Here, we propose an approach that allows us to analyze model behavior in \ac{d2t} generation on novel, real-world structured data records. We focus on identifying factual errors in the model outputs. To identify the errors, we use two separate evaluation schemes: manual annotations from human crowdworkers \cite{vanderleeHumanEvaluationAutomatically2021} and a custom automatic metric based on GPT-4 \cite{liuGEvalNLGEvaluation2023,chiang-lee-2023-large,kocmiGEMBAMQMDetectingTranslation2023}. We annotate the errors on the level of individual tokens, getting fine-grained annotations of error spans for each category \cite{thomsonGoldStandardMethodology2020,thomson2023evaluating}. We also push towards better reproducibility by \textit{focusing on open \acp{llm}}, which -- apart from being more accessible -- also achieve increasingly better results across tasks \cite{zheng2023judging,open-llm-leaderboard}. For our experiments, we use three open \acp{llm} with 7B parameters: Llama 2 \cite{touvronLlamaOpenFoundation2023,llama-2-7b-32k}, Mistral \cite{jiangMistral7B2023}, and Zephyr \cite{tunstallZephyrDirectDistillation2023}.



% To collect the data for our experiments, we introduce \datatool\footnote{\underline{Q}uintet of \underline{U}nlabeled \underline{I}nputs for \underline{N}atural \underline{T}asks in \underline{D}ata-to-text, pronounced as ``quintet''} -- a tool for collecting structured data from five domains in standard formats: JSON,
% \footnote{\url{https://www.json.org/}}
% CSV,
% \footnote{\url{https://tools.ietf.org/rfc4180}}
% and Markdown.
% \footnote{\url{https://www.rfc-editor.org/rfc/rfc7763.html}
% We choose the domains so that the data can be directly used as input for five distinct \ac{d2t} generation tasks: generating weather forecasts, sports reports, product descriptions, chart captions, and entity descriptions (see \autoref{tab:quintd:data}).
% %
% Next, we collect a set of 1,000 inputs with \datatool and use the inputs as an ad-hoc benchmark (called \benchmark) for testing the abilities of \acp{llm} for \ac{d2t} generation. We assume that the data formats in \benchmark are common in the \acp{llm}' pretraining corpora, so we specify the task using instructions instead of standard finetuning with human-written outputs, capitalizing on the zero-shot abilities of instruction-tuned \acp{llm} (§\ref{sec:data}).

%  For our experiments, we use three open \acp{llm} with 7B parameters: Llama 2 \cite{touvronLlamaOpenFoundation2023,llama-2-7b-32k}, Mistral \cite{jiangMistral7B2023}, and Zephyr \cite{tunstallZephyrDirectDistillation2023}. We also use GPT-3.5 \cite{chatgpt} as a closed model baseline for the final experiments. Given the behavioral nature of the experiments with \acp{llm} \cite{holtzmanGenerativeModelsComplex2023}, we put emphasis on reporting model behavior throughout the process  (§\ref{sec:experiments}).

% Another piece of the puzzle is \textit{reference-free evaluation}: using the input data as a ground for comparison instead of reference outputs (§\ref{sec:eval}). We focus on identifying factual errors in the model outputs. To identify the errors, we use two separate evaluation schemes: manual annotations from human crowdworkers \cite{vanderleeHumanEvaluationAutomatically2021} and a custom automatic metric based on GPT-4 \cite{liuGEvalNLGEvaluation2023,chiang-lee-2023-large,kocmiGEMBAMQMDetectingTranslation2023}. We annotate the errors on the level of individual tokens, getting fine-grained annotations of error spans for each category \cite{thomsonGoldStandardMethodology2020,thomson2023evaluating}.

% Based on our results, we provide general recommendations about \ac{d2t} generation with open \acp{llm} across tasks and formats (§\ref{sec:discussion}). Our main findings are as follows:
% \begin{itemize}
%     \item \textbf{Open \acp{llm} can generate fluent outputs from structured data} in common formats under zero-shot settings.
%     \item \textbf{Semantic accuracy is a major obstacle}: both human annotators and GPT-4-based metric report that over 80\% of outputs of open \acp{llm} on our data contain a semantic error.
%     \item \textbf{Long data inputs cause practical issues}, including the need for long-context models, increased GPU memory requirements, and unavailability of few-shot approaches.
%     \item \textbf{Outputs can be empirically improved by following several rules-of-thumb} for preprocessing the model input, such as including units, removing unnecessary fields, or prefixing the model answer.
% \end{itemize}


\subsection{Reference-Free D2T Generation}
\label{sec:quintd:data}

\paragraph{Data Collection Tool}
\label{sec:quintd:data_collection}
We introduce a tool named \datatool\footnote{\underline{Q}uintet of \underline{U}nlabeled \underline{I}nputs for \underline{N}atural \underline{T}asks in \underline{D}ata-to-text, pronounced as ``quintet''} for collecting ad-hoc test sets using public APIs in five different domains.
% , and we collect one such set (\benchmark) for our experiments. 
Our main reasons for departing from the traditional scheme of benchmarking on well-established datasets are:
\begin{enumerate}
    \item Any published test sets may be potentially included in the training data of \acp{llm}.
    \item Public sources of structured data offer enough resources for creating ad-hoc test sets.
    \item Without human references, our data collection scheme is lightweight and replicable.
\end{enumerate}

Given the available public sources of data, we settled on the five tasks which are described in \autoref{tab:quintd:data}. The tasks are based on structured data in common formats: JSON, CSV, and Markdown.


\begin{figure}[t]
    \centering
    \small
    \textbf{Prompt}
    \begin{verbatimbox}
        Based on the given data:

        \`{}\`{}\`{}

        \{data\}

        \`{}\`{}\`{}

        Your task is to write a brief, fluent, and
        coherent single-paragraph \{output\_type\} in natural
        language. The text should be balanced and neutral.
        Make sure that all the facts mentioned in the text
        can be derived from the input data, do *not* add
        any extra information.
    \end{verbatimbox}
    \textbf{Output prefix}
    \begin{verbatimbox}
        Sure! Here is the \{output\_type\}:

        "
    \end{verbatimbox}
    \caption{The prompt $\mathcal{P}$ and the model output prefix we used for the experiments in this paper. \texttt{data} is filled with the data record  $x$ and \texttt{output\_type} is filled accordingly for each domain $\mathcal{D}$ (see \autoref{tab:quintd:data}).}
    \label{fig:quintd:prompt}
\end{figure}


\paragraph{\benchmark{} Dataset}
\label{sec:quintd:dataset}
Using \datatool{}, we collected the dataset for our experiments in this paper (\datatool{}-1). The dataset contains 500 examples in the development set and 500 examples in the test set (100 examples per domain for each split).
%  We kept the size of the dataset moderate for a quick experimental turnaround.
We downloaded the data between November 2023 and January 2024. Note that the dataset contains only \emph{unlabeled} data without any reference outputs (e.g., weather data, but not a textual weather forecast), so the outputs need to be evaluated using reference-free metrics.



\paragraph{Task Definition}
Each example in \benchmark{} consists of a structured data record $x$ from a domain $\mathcal{D} \in $ \{\texttt{openweather}, \texttt{gsmarena}, \texttt{ice\_hockey}, \texttt{owid}, \texttt{wikidata}\}. Given $x$ and a prompt $\mathcal{P}$, the goal is to generate natural language output $y$ faithful to the data $x$, according to the instructions in the prompt $\mathcal{P}$ (see \autoref{fig:quintd:prompt}).




% Since diversity and controllability of the output style is generally an underexplored topic in \ac{d2t} generation (although see \citealp{hanGeneratingDiverseDescriptions2021}; \citealp{perlitzDiversityEnhancedTabletoText2022}), we cover three scenarios with different prompts, focusing on \textit{stylized} and \textit{topic-centric generation}:
% \begin{itemize}
%   \item $\mathcal{P}_\text{direct}$ -- Generating \texttt{output\_type} in a neutral and balanced way.
%   \item $\mathcal{P}_\text{stylized}$ -- Generating \texttt{output\_type} written in a specific style (\texttt{style}).
%   \item $\mathcal{P}_\text{topic-centric}$ -- Generating \texttt{output\_type} focusing on a given aspect (\texttt{aspect}).
% \end{itemize}

% We selected the values of the style and aspect variables manually for each domain to accomodate for their specific purposes and data formats (see Table~\ref{tab:setups}).


\subsection{Experiments}
\label{sec:quintd:experiments}

% https://drive.google.com/file/d/1vMj_to9BHdeX0BgD2eX_rN--Q2kl2uct/view?usp=sharing
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{img/quintd_process.pdf}
    \caption{Our experimental setup. We first generate the outputs using \acp{llm} that are given raw data and a task-specific prompt. We annotate the token-level semantic errors in the LLM outputs with (a) an automatic metric based on GPT-4 that matches the output to the raw data, and (b) human annotators, who annotate the errors in the output given the data visualization.}\label{fig:quintd:process}
\end{figure*}

\paragraph{Models}
\label{sec:quintd:models}
For our experiments, we selected the following \acp{llm} available under an open license:

\begin{itemize}
    \item \textbf{Llama 2} \cite{touvronLlamaOpenFoundation2023,llama-2-7b-32k} -- \texttt{\small together\-computer\-/Llama-2-7B\--32K-Instruct}
    \item \textbf{Mistral} \cite{jiangMistral7B2023} -- \texttt{\small mistralai/Mistral-7B-Instruct-v0.1}
    \item \textbf{Zephyr}  \cite{tunstallZephyrDirectDistillation2023} -- \texttt{\small HuggingFaceH4/zephyr-7b-beta}
\end{itemize}

The models are instruction-tuned, operate with 32k context, and perform well on recent benchmarks. All the models have 7B parameters and thus fit on a single NVIDIA A40 (48G VRAM) in 16-bit precision. We accessed the models via an API provided by the \texttt{text-generation-webui} framework\footnote{\url{https://github.com/oobabooga/text-generation-webui}} running locally.
For the final experiments, we also included GPT-3.5 (\texttt{gpt-3.5-turbo-1106}) accessed through the OpenAI API \cite{chatgpt}.\footnote{We only included GPT-3.5 in our final experiments since proprietary models were not our focus. We did not include GPT-4 since we use the model for evaluation (see §\ref{sec:quintd:gpt4eval}) and LLMs tend to be biased towards their outputs \cite{kooBenchmarkingCognitiveBiases2023,stureborgLargeLanguageModels2024}.}

\paragraph{Experimental Process}
\label{sec:quintd:process}
To extensive prompt engineering, we use the same prompt template $\mathcal{P}$ for all the domains and models. For our preliminary experiments, we first wrote down the initial version of the prompt and used the data without further preprocessing.
We then iteratively improved our experimental setup by observing outputs on the development set.


\paragraph{Observations from Preliminary Experiments}
\label{sec:quintd:observations}
Here, we describe all the observations and modifications we made before generating the final outputs on the test set:

\begin{itemize}


    \item\textbf{Any input field may appear in the output.} The models do not always select the most relevant fields for the given output. For example, we observed that the models commonly mention identifiers, timestamps, files, and other metadata, leading to unnatural outputs. To mitigate these issues, we manually filtered out the irrelevant fields from the input.

    \item\textbf{Units need to be specified explicitly.} If the units are not specified in the data record, the models tend to resort to their best guess. This may go unnoticed if the unit is evident from the context (e.g., the model will usually not report the temperature in Fahrenheit instead of Celsius), but it may get problematic if the value is ambiguous (e.g., wind speed in km/h versus m/s). Therefore, we explicitly add units to all data records where appropriate.

    \item\textbf{Understandable keys are enough.} On the flip side, we decided not to add extra descriptions to the keys if the key was understandable from its name (e.g., \texttt{homeTeam} or \texttt{dimensions}). As we discussed in \autoref{sec:rel2text}, pretrained models interpret field names correctly as long as they are human-readable. We only include chart metadata for the CSV files in the \texttt{owid} domain.

    \item\textbf{Long inputs can be troublesome.} The inputs in some domains can easily get longer than 10-20k tokens. This issue is amplified by the fact that the evaluated LLMs tokenize numbers into individual digits. To accommodate for the long inputs, we picked models that accept up to 32k tokens.\footnote{For this reason, we use Llama-2-7B-32k with 32k token context \cite{llama-2-7b-32k} instead of the official Llama-2-7B-Instruct, which only supports 4k context \cite{touvronLlamaOpenFoundation2023}.} However, with long inputs, the GPU memory consumption also gets considerably higher, so we needed to downsample the data in \texttt{owid} and \texttt{openweather} to keep their length under \textasciitilde 8k tokens.


    \item\textbf{Few-shot experiments are infeasible.} Due to the context-length limitations, we were not able to run few-shot experiments since we could not robustly fit an additional ($x_\text{example}$, $y_\text{example}$) pair in the prompt. We attempted to include only $y_\text{example}$ (making the setup ``half-shot''), but we observed that the models then used entities from the example (unrelated to the actual input) in their outputs. Therefore, we decided to leave this line of experiments for future work (see \autoref{sec:quintd:future} for discussion).

    \item\textbf{Deterministic decoding and sampling are on par.} In our preliminary experiments, we observed a roughly similar output quality for both deterministic decoding and sampling.\footnote{We used the \texttt{text-generation-webui} default decoding parameters: \texttt{temperature}=0.7, \texttt{top\_p}=0.9, and \texttt{top\_k}=20.} For the final experiments, we decided to use deterministic decoding, which is non-parametric and conceptually more suitable for D2T generation.

    \item\textbf{Prefixing the output makes parsing easier.} Even with variations of a \textit{``generate only the output''} instruction appended to the prompt, the models (especially Llama 2) tended to first confirm the request. For that reason, we decided to prefix the input for all the models with \textit{``Sure! Here is the \{output\_type\}: "''}. The opening quote at the end of the prefix allowed us to robustly parse the text simply by stripping the closing quote from the model output.

    \item\textbf{The outputs are fluent but inaccurate.} We observed that the vast majority of model outputs were grammatically and stylistically correct, capturing the output type specified in the prompt. However, we also noticed that the outputs contained many factual errors (even after emphasizing the focus on factual accuracy in the prompt, see \autoref{fig:quintd:prompt}). This observation led us to evaluate the model outputs using word-level annotations focused on semantic accuracy errors (see \autoref{sec:quintd:eval}).


    \item\textbf{Be careful about subtle bugs.} During our preliminary experiments, we had to fix subtle bugs in our API calls such as incorrect instruction templates\footnote{\url{https://huggingface.co/docs/transformers/chat_templating}} or involuntary input truncation. We note that with the apparent ease of API access and robustness of LLMs, such bugs could go unnoticed and artificially skew the model performance.

\end{itemize}



\paragraph{Final Experiments}
\label{sec:quintd:basic}

Taking the observations from our preliminary experiments into account, we proceeded to generate the outputs on the test set of \benchmark{} for token-level error analysis. We first preprocessed the data as mentioned: we stripped out unnecessary fields, added units, and downsampled the data to fit the context. For all the models, we used the prompt in \autoref{fig:quintd:prompt} and deterministic decoding with a maximum length of 512 tokens.

For comparison, we also generated outputs for the same inputs and identical prompts with GPT-3.5. Note that even though we fixed the temperature and seed to $0$, the rest of the decoding parameters are inaccessible to us and may differ from the parameters we used for the open models.


\subsection{Evaluation}
\label{sec:quintd:eval}
For evaluation, we focus on identifying \emph{factual errors} in model outputs. We compare the generated texts to the input data, looking for parts of texts that are not faithful to the input data. We annotate the errors on the token level, considering all the tokens in the output text as potential sources of errors.


We use two complementary referenceless evaluation methods:
\begin{itemize}
    \item \humanmetric{}: \textbf{human evaluation} based on crowdsourcing,
    \item \gptmetric{}: \textbf{an automatic metric} based on GPT-4.
\end{itemize}

The methods use similar instructions and produce outputs with equivalent format. The main idea is to compensate for the shortcomings of each approach: while human evaluation is costly (about 10$\times$ more expensive than automatic evaluation), using only an automatic metric based on a closed LLM would make the evaluation potentially non-reproducible and biased \cite{kocmiGEMBAMQMDetectingTranslation2023,wangLargeLanguageModels2023}. Assuming that the methods are interchangeable, reporting the results of both methods should hopefully increase the robustness and replicability of our results.




Our error taxonomy and its notation are inspired by \citet{thomsonGoldStandardMethodology2020}, similarly as in \autoref{sec:tok-eval}. After preliminary examination of the outputs, we settled on four error categories: \errinc{INCORRECT}, \errnc{NOT\_CHECKABLE}, \errmis{MISLEADING}, and \errother{OTHER}. To set a clear boundary between the categories and reach better inter-annotator agreement, we decided to keep our taxonomy more high-level this time and not to distinguish between fine-grained categories (e.g., \textit{incorrect name} vs. \textit{incorrect number}). The descriptions of our error categories, as presented in the instructions for annotation, are included in \autoref{tab:quintd:errors}.

\begin{table*}[t]
    \footnotesize
    \centering
    \begin{tabular}{p{3.5cm}p{10cm}} \toprule                                                                                           \\
        \textbf{Error}         & \textbf{Description}                                                                        \\ \midrule
        \errinc{INCORRECT}     & The fact in the text contradicts the data.                                                  \\
        \errnc{NOT\_CHECKABLE} & The fact in the text cannot be checked given the data.                                      \\
        \errmis{MISLEADING}    & The fact in the text is misleading in the given context.                                    \\
        \errother{OTHER}       & The text is problematic for another reason, e.g., grammatically or stylistically incorrect,
        irrelevant, or repetitive.                                                                                           \\\midrule

        \textbf{Example}       &                                                                                             \\

        \textit{data}          &
        \textbf{Nokia 3310} |
        \textit{color}: black, blue, grey |
        \textit{display}: 320x240px                                                                                          \\
        \textit{text}          &
        Nokia 3310 is \errnc{produced in Finland} and features a \errinc{320x320} display. It is \errmis{available in black color}. \errother{The data seem to provide only partial information about the phone.}
        \vspace*{0.1cm}
        \\ \bottomrule
    \end{tabular}
    \caption{Categories of errors annotated in our evaluation and an example demonstrating the error types.}
    \label{tab:quintd:errors}
\end{table*}


\paragraph{Human-based Evaluation}
\label{sec:quintd:humaneval}

% An automatic metric based on a closed LLM makes the evaluation potentially non-reproducible and biased \cite{kocmiGEMBAMQMDetectingTranslation2023,wangLargeLanguageModels2023}, for which we compensate by obtaining annotations from human annotators.

For the human annotation metric, we prepared a custom web interface using \texttt{factgenie}\footnote{\url{https://github.com/kasnerz/factgenie}}. In the web interface, an annotator is instructed to annotate text spans with respective error categories. We created custom visualizations for each data format, as illustrated in \autoref{fig:quintd:process}.

We hired annotators on the Prolific crowdsourcing platform.\footnote{\url{https://prolific.com}} In total, we hired 100 annotators, each annotating 20 examples (4 model outputs for each of the five domains). We selected annotators with at least 10 completed tasks and a 100\% approval rate, having English as their primary language. We paid the annotators \textsterling{}9 per hour, according to the platform's recommendations. The median time for completing the annotations was 47 minutes.




\paragraph{GPT-4-based Evaluation}
\label{sec:quintd:gpt4eval}

For automatic evaluation, we leverage the fact that LLM-based metrics can be customized for a particular task without the need for training data. In our experiments, we use a metric based on GPT-4 (\texttt{gpt-4-1106-preview}, \citealp{openai2023gpt4}), which was shown to be superior to other \acp{llm} in following fine-grained instructions and have high correlations with human judgment on evaluating generated texts \cite{zhaoInvestigatingTabletoTextGeneration2023,sottanaEvaluationMetricsEra2023,kocmiGEMBAMQMDetectingTranslation2023,kocmiLargeLanguageModels2023}.
% Compared to using fine-tuned models for token-level evaluation \cite{kasnerTextinContextTokenLevelError2021}, this allows us to customize the metric for a particular task without the need for training data.

We instantiate \gptmetric{} with a prompt and a system message describing the task. We instruct the model to produce a JSON output with sequentially ordered errors using the following format:

\small
\begin{verbatim}
{
  "errors": [{
    "reason": [REASON],
    "text": [TEXT_SPAN],
    "type": [ERROR_CATEGORY]
    }, 
    ...]
}.
\end{verbatim}
\normalsize


Note that we require that the model first generates the free-form text \textit{reason} for the error.\footnote{We did not ask the crowdworkers for free-form reasoning about the errors since that would make the annotation notably more complex.} Generating the reason comes at almost no extra cost and our cursory observations suggest that requiring it leads to more precise outputs.

We align the model outputs with the original text by string matching on \texttt{TEXT\_SPAN}, moving the current position forward after each match. We ensure that the model response is a valid JSON using OpenAI's \texttt{response\_format} parameter.




% \paragraph{Evaluating Controllability}

% Due to our resource limitations, we were not able to run the token-level error annotations for all the stylized outputs. Instead, we randomly selected 10 outputs for each domain on which we run the GPT-4 annotations, and two annotators (authors of the paper) rated the adherence to style along with free-form comments.

\subsection{Results and Discussion}
\label{sec:quintd:discussion}

A summary of the token-level annotations is in \autoref{tab:quintd:results_agg} and \ref{tab:quintd:results_errperex}.


\begin{table*}[ht]
    \footnotesize
    \centering
    \begin{tabular}{lccccccccccr}
        \toprule
                         & \multicolumn{2}{c}{\textbf{Incorrect}} & \multicolumn{2}{c}{\textbf{Not Checkable}} & \multicolumn{2}{c}{\textbf{Misleading}} & \multicolumn{2}{c}{\textbf{Other}} & \multicolumn{2}{c}{\textbf{All cat.}} &                                                                                                 \\
                         & \humanmetric{}                         & \gptmetric{}                               & \humanmetric{}                          & \gptmetric{}                       & \humanmetric{}                        & \gptmetric{}  & \humanmetric{} & \gptmetric{}  & \humanmetric{} & \gptmetric{}  & \textbf{Tok.} \\\midrule
        \textbf{Llama 2} & 1.57                                   & \textbf{2.79}                              & 1.25                                    & 0.91                               & 0.25                                  & \textbf{0.12} & \textbf{0.10}  & 0.09          & 3.18           & 3.90          & 83.8          \\
        \textbf{Mistral} & 2.03                                   & 3.23                                       & 1.12                                    & 0.54                               & 0.44                                  & 0.26          & 0.25           & 0.10          & 3.85           & 4.12          & 114.9         \\
        \textbf{Zephyr}  & \textbf{1.44}                          & 2.84                                       & \textbf{0.77}                           & \textbf{0.40}                      & \textbf{0.20}                         & 0.29          & 0.16           & \textbf{0.05} & \textbf{2.58}  & \textbf{3.58} & 98.0          \\ \cdashlinelr{1-12}
        \textbf{GPT-3.5} & 0.65                                   & 1.76                                       & 0.49                                    & 0.38                               & 0.18                                  & 0.26          & 0.07           & 0.02          & 1.39           & 2.42          & 84.9          \\ \bottomrule
    \end{tabular}
    \caption{The average \textit{numbers of errors per output} (lower is better) based on human annotators (\humanmetric{}) and GPT-4 (\gptmetric{}). We also include the average number of tokens per output in the rightmost column. The results of the best open LLM  are emphasized.}
    \label{tab:quintd:results_agg}
\end{table*}


\begin{table*}[ht]
    \footnotesize
    \centering
    \begin{tabular}{lrrrrrrrrrr}
        \toprule
                         & \multicolumn{2}{c}{\textbf{Incorrect}} & \multicolumn{2}{c}{\textbf{Not Checkable}} & \multicolumn{2}{c}{\textbf{Misleading}} & \multicolumn{2}{c}{\textbf{Other}} & \multicolumn{2}{c}{\textbf{All cat.}}                                                                                        \\
                         & \humanmetric{}                         & \gptmetric{}                               & \humanmetric{}                          & \gptmetric{}                       & \humanmetric{}                        & \gptmetric{}   & \humanmetric{} & \gptmetric{}   & \humanmetric{}  & \gptmetric{}    \\\midrule
        \textbf{Llama 2} & 53.2\%                                 & 80.0\%                                     & 57.4\%                                  & 44.8\%                             & 17.4\%                                & \textbf{8.8\%} & \textbf{7.6\%} & 7.6\%          & 85.6\%          & 94.0\%          \\
        \textbf{Mistral} & 53.6\%                                 & 80.2\%                                     & 49.6\%                                  & 31.8\%                             & 20.6\%                                & 17.0\%         & 13.6\%         & 8.4\%          & 81.2\%          & 93.0\%          \\
        \textbf{Zephyr}  & \textbf{46.8\%}                        & \textbf{78.0\%}                            & \textbf{42.2\%}                         & \textbf{25.0\%}                    & \textbf{16.2\%}                       & 20.6\%         & 11.6\%         & \textbf{4.2\%} & \textbf{75.6\%} & \textbf{89.4\%} \\\cdashlinelr{1-11}
        \textbf{GPT-3.5} & 38.0\%                                 & 65.0\%                                     & 28.8\%                                  & 19.6\%                             & 12.6\%                                & 16.2\%         & 6.2\%          & 2.2\%          & 60.6\%          & 75.8\%          \\\bottomrule
    \end{tabular}
    \caption{The percentage of \textit{outputs containing at least one error} (lower is better) based on human annotators (\humanmetric{}) and  GPT-4 (\gptmetric{}). The results of the best open LLM  are emphasized.}
    \label{tab:quintd:results_errperex}
\end{table*}



\paragraph{How Accurate Are the Model Outputs?}
Depending on the model, between 76-86\% of examples contain an error according to \humanmetric, suggesting that open \acp{llm} make semantic errors very often. According to \gptmetric, the number is as high as 89-94\%.

The most common error type is \errinc{INCORRECT}. As shown in \autoref{tab:quintd:results_agg}, all the open \acp{llm} make more than \textbf{two statements contradicting the data per output on average}. The \errnc{NOT\_CHECKABLE} errors are also relatively common: more than one per output on average according to \humanmetric, and at least one being present in more than 25\% of examples according to both metrics.

% The results vary widely according to the domain (see Appendix \ref{app:full_results}). For example, the outputs in \texttt{wikidata} contain much more \errnc{NOT\_CHECKABLE} errors on average (1.01 per output according to \humanmetric) than  \errinc{INCORRECT} errors (0.11 per output according to \humanmetric), suggesting that with simpler inputs, the models tend to introduce extra information. The \texttt{openweather} domain seems to be the most complex with the longest outputs (\textasciitilde 164 tokens), more than eight errors in the output on average, and >90\% of outputs containing an error.

The differences between the open \acp{llm} are not major. Out of the open \acp{llm}, Zephyr has the best results across categories and metrics, followed by Llama 2. However, the outputs of Mistral are longer on average, leaving more space for errors. GPT-3.5 (which we consider separately) does generally better according to both \gptmetric and \humanmetric, although it still makes an error in 60-75\% of examples (2 errors per example on average). In general, the results show that \acp{llm} make too many semantic errors to be usable in practice for \ac{d2t} generation in a zero-shot setting.



\paragraph{Do Evaluation Methods Agree?}
\label{sec:quintd:metricscorr}
To quantify the agreement of \humanmetric{} and \gptmetric{}, we computed the Pearson correlation coefficient between the error counts on the level of tokens, examples, and domains as follows (note that each error category was considered separately):
\begin{itemize}
    \item For $r_{\text{domain}}$, we used the average error counts per domain.
    \item For $r_{\text{example}}$, we used the count of errors per example.
    \item For $r_{\text{token}}$, we used the binary indicators marking an error per token.
\end{itemize}
The correlation on the level of tokens is weak ($r_{\text{token}}=0.26$) but gets better on the example-level ($r_{\text{example}}=0.52$) and even better on the domain-level ($r_{\text{domain}}=0.93$). In \autoref{tab:quintd:errors_tokenlevel}, we show the percentage of tokens marked by individual metrics. The metrics agree on the specific tokens in less than 6\%, although they both mark around 21\% of tokens as erroneous.
% , suggesting that the error spans may be subject to interpretation.

We also measure inter-annotator agreement between human annotators. For that, we obtained annotations from two annotators for 100 model outputs. The results are similar: the annotators agree weakly on the token level ($r_{\text{token}}=0.36$), stronger on the example level ($r_{\text{example}}=0.53$), and even stronger on the domain level ($r_{\text{domain}}=0.85$). We conclude that while the details regarding error spans and categories may vary, the annotators as well as GPT-4 generally agree on the accuracy of model outputs for a given set of examples. In the future, the agreement could be improved by measuring errors on the phrase level \cite{vamvas2022little}.


\paragraph{Recommendations for Future Work}
\label{sec:quintd:future}

\begin{itemize}
    \item \textbf{Focus on semantic accuracy.} The output of \acp{llm} is satisfactory regarding the style, format, and purpose of the text. However, the amount of semantic errors remains very high. Improving the semantic accuracy of the models
          \cite{li2022faithfulness}, along with new model-based evaluation metrics \cite{liuGEvalNLGEvaluation2023,xuINSTRUCTSCOREExplainableText2023}, could thus help to bring improve LLM-based \ac{d2t} generation systems where it is most needed.
          \item\textbf{Use long-context models.} The memory issues with long context, making few-shot experiments infeasible, can potentially be solved by using more efficient long-context models equipped with Flash Attention \cite{dao2022flashattention} and fast inference libraries such as \texttt{llama.cpp}\footnote{\url{https://github.com/ggerganov/llama.cpp}}, especially in the light of the recent rapid increase of the available context window size of \acp{llm} \cite{bai2023longbench,munkhdalai2024leave}.
          \item\textbf{Test the models in the wild.} Except for using an ad-hoc dataset of real-world data as we did in our work, the ecological validity of D2T evaluation can also be ensured by continuous evaluation with human users \cite{zheng2023judging} and evaluating the real-world impact of the systems \cite{reiter2023impact}.
          \item\textbf{Multilinguality is an opportunity.} With the recent efforts in extending \ac{d2t} generation to low-resource languages \cite{cripwell2023WebNLGShared2023}, multilingual \ac{d2t} generation with open \acp{llm} seems a promising direction. Although we did not go beyond English, initial steps were already done by works such as \citet{lorandi2023data}.
\end{itemize}

\section{Conclusion}
In \autoref{sec:quintd}, we provided an exploratory study into D2T generation with open LLMs. We proposed new directions for D2T generation, including using ad-hoc test sets, data in common formats, and reference-free evaluation. By a combination of GPT-4-based metric and human evaluation, we evaluated the performance of LLMs on five domains, providing word-level annotations of model outputs across five domains and recommendations for future directions in D2T generation.








