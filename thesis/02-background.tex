
\chapter{Background}
\label{chap:background}

This chapter explains the \textbf{basic concepts} used throughout the thesis. First, we explain \textbf{neural \acp{lm}}: from the basic of neural networks (§\ref{sec:nns}) and language modeling (§\ref{sec:lm-basics}), up to pretrained (§\ref{sec:plms}) and large language models (§\ref{sec:llms}). Next, we move on to \textbf{\ac{d2t} generation}: covering rule-based (§\ref{sec:rule-d2t}) and neural-based systems (§\ref{sec:neural-d2t}), \ac{d2t} datasets (§\ref{sec:datasets}) and evaluation methods (§\ref{sec:evaluation}). We assume that the reader has certain expertise in related areas of \ac{nlp}, although not necessarily in \ac{nlg}. We also aim to make the work self-contained by covering all the related concepts, but the explanations are brief, and the interested reader is referred to the respective works for details.

Besides explaining the basic concepts, the chapter serves also as an \textbf{overview of the state of the art} in the field. In particular, the later subsections (\Cref{sec:plms,sec:llms} for neural \acp{lm} and \Cref{sec:neural-d2t,sec:datasets,sec:evaluation} for \ac{d2t} generation) focus on summarizing related work, providing pointers to it, and describing the datasets and models used for the experiments. As such, the chapter serves as a main reference for the related work for later chapters, and we will only briefly revisit the most relevant works in the respective chapters.


\section{Neural Language Models}
\label{sec:lms}
In this section, we work our way towards neural \acp{lm}: the mathematical foundations of \acp{nn} the \acp{lm} are built on, and the way \acp{lm} are constructed, trained, and eventually applied in \ac{nlp}.

\subsection{Neural Networks}
\label{sec:nns}
Let's assume our goal is to predict the real-number output $y \in \mathbb{R}$ for the given real vector $\mathbf{x} \in \mathbb{R}$.\footnote{We will stick to the convention that vectors are denoted with boldface, e.g. $\mathbf{x}$, and real numbers with regular font, e.g. $x$.} For an arbitrary $\mathbf{x} \rightarrow y$ mapping, that is quite an ambitious goal. To make the task easier, let's also assume that $\mathbf{x}$'s and $y$'s are representations of some causally connected real-world data, implying there is a certain non-trivial dependence of values of $y$ on $\mathbf{x}$ which can be learned.
%  and come from naturally occuring distributions $\mathbf{x} \in \mathcal{X}$, $y \in \mathcal{Y}$. 
% This suggest there are regularities and underlying patterns in the data and .

For learning the dependence (which will, in turn, allow us to make predictions about the real-world phenomena represented by $\mathbf{x}$'s and $y$'s), we can use mathematical models designed to capture the underlying rules in their parameters. The idea is that the models estimate the parameters from a limited set of examples called the \textit{training data}:  $\mathcal{D_{\text{train}}} = \{(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_{n}, y_{n})\}$, and use the learned parameters to predict the outputs on the \textit{test data}: $\mathcal{D_{\text{test}}} = \{(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_{m}, y_{m})\}$.

\paragraph{Perceptron Algorithm} One of the early mathematical models designed for predicting the outputs (in this case binary, $y \in \{-1, 1\}$) based on the input data is the perceptron algorithm. The algorithm learns parameters $\textbf{w}$ and $b$ describing a linear decision boundary, separating the data points $\mathbf{x_i}$ so that the points $\{\mathbf{x_i} | y_i = -1\}$ lay on one side of the boundary and the points $\{\mathbf{x_j} | y_j = 1\}$ on another. The algorithm proceeds as follows:




\begin{enumerate}
    \item The parameters $\textbf{w}$ and $b$ are initialized to small random values (or zeros).
    \item For each training example $(\mathbf{x}_i, y_i)$:
          \begin{itemize}
              \item Compute the predicted output $\hat{y}_i$ using the current weights and bias: $\hat{y}_i = \text{sign}(\textbf{w} \cdot x_i + b)$.
              \item Update the weights and bias using the perceptron update rule:
                    \[ w = w + (y_i - \hat{y}_i) x_i \]
                    \[ b = b + y_i - \hat{y}_i \]
          \end{itemize}
    \item Repeat until convergence.
\end{enumerate}

The perceptron algorithm is guaranteed to converge if the data is linearly separable, i.e., if the data can be separated with a hyperplane.



\subsection{Language Models}
\label{sec:lm-basics}
\subsection{Transformer Architecture}
\label{sec:transformer}
\subsection{Pretrained Language Models}
\label{sec:plms}
\subsection{Large Language Models}
\label{sec:llms}
\section{Data-to-Text Generation}
\label{sec:d2t}
\subsection{Rule-based Approaches}
\label{sec:rule-d2t}
\subsection{Neural Approaches}
\label{sec:neural-d2t}
\subsection{Datasets}
\label{sec:datasets}
\subsection{Evaluation Metrics}
\label{sec:evaluation}