%%% The main file. It contains definitions of basic parameters and includes all other parts.

%% Settings for single-side (simplex) printing
% Margins: left 40mm, right 25mm, top and bottom 25mm
% (but beware, LaTeX adds 1in implicitly)
\documentclass[12pt,notitlepage,a4paper,openright]{report}
\pagestyle{plain}
\usepackage{url}
\def\UrlBreaks{\do\/\do-}
\PassOptionsToPackage{hyperfootnotes=false}{hyperref}

% fix pdfx
\usepackage{etoolbox}
% \makeatletter
% \@ifl@t@r\fmtversion{2021-06-01}%
%  {\AddToHook{package/after/xmpincl}
%    {\patchcmd\mcs@xmpincl@patchFile{\if\par}{\ifx\par}{}{\fail}}}{}
% \makeatother

\usepackage[usenames,dvipsnames,svgnames,table,rgb]{xcolor}
\usepackage[a-2u]{pdfx}
\usepackage{fontspec}
\usepackage[czech,english]{babel}
\usepackage{lmodern}
\usepackage{textcomp}
\usepackage[defaultlines=4,all]{nowidow}

% Turn this on when needed:
%\usepackage{microtype}

\usepackage{graphicx}
\usepackage[twoside, inner=3.7cm, outer=2.9cm, top=2.6cm, bottom=3.4cm]{geometry}
\usepackage{thesis}
\usepackage[round]{natbib}
\usepackage{multirow}
\usepackage{arydshln} % dashed lines in tables
\usepackage{array}
\usepackage{amssymb,latexsym,pifont}
\usepackage{amsmath}
\usepackage{enumitem} % custom lists
\usepackage[normalem]{ulem} % underlining
\usepackage{setspace} % line spacing
\usepackage{varioref} % nice references (above/below)
\usepackage[above,section]{placeins} % avoid figures pushed at end of chapters
\usepackage{listings}

\setlist[itemize]{itemsep=0.02cm,topsep=0.2cm}
\setlist[enumerate]{itemsep=0.02cm,topsep=0.2cm,label={(\arabic*)}}

\usepackage{tabularx}
\usepackage{booktabs} % nicer lines in table
\usepackage{multicol}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}
\usepackage{gnuplot-lua-tikz}
\usetikzlibrary{shapes.geometric}
\usepackage{epstopdf}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathtools}
\usepackage{quoting,xparse}


% \usepackage[parfill]{parskip}

% acronyms and glossaries
\usepackage[acronym, nomain]{glossaries}
\usepackage[shortcuts=ac]{glossaries-extra}
\makeglossaries
\preto\chapter{\glsresetall}

\setabbreviationstyle[acronym]{long-short}

\usepackage{subcaption} % sub figures in a fiture
\usepackage{standalone} % include standoalone tikz images
\usepackage{bibentry}

% hack bibentry command for list of publications
\makeatletter
\renewcommand\bibentry[1]{\nocite{#1}{\frenchspacing
     \@nameuse{BR@r@#1\@extra@b@citeb}}}
\makeatother

\renewcommand{\chapterautorefname}{Chapter}
\renewcommand{\sectionautorefname}{Section}
\renewcommand{\subsectionautorefname}{Section}
\renewcommand{\subsubsectionautorefname}{Section}



\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\hypersetup{ %
  colorlinks=true,
  linkcolor=black,
  citecolor=mydarkblue,
  filecolor=mydarkblue,
  urlcolor=mydarkblue,
  unicode=true
}

% \hypersetup{
%     colorlinks=false,
%     pdfborder={0 0 0},
%     unicode=true,
% }

\input{acronyms}

% Czech babel conflicts with cline, hacky fix (http://tex.stackexchange.com/questions/111999/slovak-and-czech-babel-gives-problems-with-cmidrule-and-cline):
% - basically disables hyphenation in tables, but it's not used anyway so it doesn't matter
\preto\tabular{\shorthandoff{-}}
\preto\tikzpicture{\shorthandoff{-}}
%
%
\hyphenation{%
da-ta-sets
da-ta-set
} % -- custom hyphenation

\setmainfont[Ligatures=Common]{Libertinus Serif}
% \setmainfont[Ligatures=Common]{Linux Libertine O}
\setsansfont[Scale=MatchLowercase]{DejaVu Sans}
\setmonofont[Scale=MatchLowercase]{DejaVu Sans Mono}


\NewDocumentCommand{\bywhom}{m}{% the Bourbaki trick
  {\nobreak\hfill\penalty50\hskip1em\null\nobreak
   \hfill\mbox{\normalfont(#1)}%
   \parfillskip=0pt \finalhyphendemerits=0 \par}%
}

\NewDocumentEnvironment{pquotation}{m}
  {\begin{quoting}[
     indentfirst=true,
     leftmargin=\parindent,
     rightmargin=\parindent]\itshape}
  {\bywhom{#1}\end{quoting}}

\setstretch{1.1} % line spacing

\expandafter\def\expandafter\quote\expandafter{\quote\small} % smaller quotations font


% orphan & widow control
%\clubpenalty 10000
%\widowpenalty 10000

% gaps between text and footnotes
\def\footnoteskip#1{
  \renewcommand\footnoterule{
     \vspace{#1}
     \hrule width 0.4\columnwidth%
     \vspace{3pt}
}
}
\footnoteskip{0.8em}


\setcounter{tocdepth}{2}
\setcounter{secnumdepth}{2}

%% cutting down warnings
%\hfuzz=2pt
%\hbadness=10000

% force-ordering citations according to dummy keys
\newcommand{\dummybiborderkey}[1]{}

\input{macros}

\newcommand{\veryshortarrow}[1][3pt]{\mathrel{%
     \vcenter{\hbox{\rule[-.5\fontdimen8\textfont3]{#1}{\fontdimen8\textfont3}}}%
     \mkern-4mu\hbox{\usefont{U}{lasy}{m}{n}\symbol{41}}}}

\newcommand{\paperdisclaim}[1]{%
\begin{center}\begin{minipage}{0.9\textwidth}
\footnotesize\it #1
\end{minipage}\end{center}
}

\def\ignorecolumn#1\unskip{}

\title{Data-to-Text Generation with Neural Language Models}
% \title{Techniques for Neural Data-to-Text Generation}

\def\fulldate{}
\author{Zdeněk Kasner}
\date{2024}
\dept{Institute of Formal and Applied Linguistics}
\supervisor{Mgr. et Mgr. Ondřej Dušek, Ph.D.}
\studyprogram{Computational Linguistics}
% \studyfield{}


\begin{document}

%
%
%
\renewcommand{\thepage}{\roman{page}}
\renewcommand\cite{\citep}
\selectlanguage{english}
\maketitle

\pagestyle{plain}
\normalsize
\setcounter{page}{2}

\cleardoublepage{}
\ \vspace{10mm}

\noindent \it

\vspace{\fill}
\noindent \rm
I declare that I carried out this doctoral thesis independently,
and only with the cited sources, literature and other professional sources.

I understand that my work relates to the rights and obligations
under the Act No.~121/2000 Coll., the Copyright Act, as amended,
in particular the fact that Charles University has the right
to conclude a license agreement on the use of this work as a school work
pursuant to Section~60 paragraph~1 of the Copyright Act.

\vspace{2cm}
\noindent Prague, \today \hspace{\fill}\theauthor % doplňte patřičné
% datum, jméno a
% příjmení

%%%   Do not forget to SIGN the printed book here!
%%%                  *********


\cleardoublepage{} % new page
\pagestyle{plain}

\addcontentsline{toc}{chapter}{English Abstract}

%\selectlanguage{english}
\begin{description}[leftmargin=7.5em,labelwidth=7em,labelindent=0em,labelsep=0.5em]
  \item[Title:] \thetitle{}
  \item[Author:] \theauthor{}
  \item[Department:] \thedept{}
  \item[Supervisor:] \thesupervisor{},\\ \thedept{}
\end{description}
\subsubsection{Abstract:}

\input{abstract_en}

\begin{description}[leftmargin=7.5em,labelwidth=7em,labelindent=0em,labelsep=0.5em]
  %
  \item[Keywords:] TODO
    %
\end{description}


\cleardoublepage{}
\addcontentsline{toc}{chapter}{Czech Abstract}
\selectlanguage{czech}
\begin{description}[leftmargin=7.5em,labelwidth=7em,labelindent=0em,labelsep=0.5em]
  \item[Název práce:] TODO
  \item[Autor:] \theauthor{}
  \item[Katedra:] Ústav formální a aplikované lingvistiky
  \item[Vedoucí práce:] \thesupervisor,\\ Ústav formální a aplikované lingvistiky
\end{description}

\subsubsection{Abstrakt:}

\input{abstract_cs}

\begin{description}[leftmargin=7.5em,labelwidth=7em,labelindent=0em,labelsep=0.5em]
  %
  \item[Klíčová slova:] TODO
    %
\end{description}

\selectlanguage{english}




\cleardoublepage{}
\ \vspace{10mm}

\addcontentsline{toc}{chapter}{Acknowledgements}
\subsection*{Acknowledgements}

{

  TODO
  % Here, you can thank anyone and say anything.

  %   \vspace{1\baselineskip}
  %   \noindent
  %   This is how I separated different kinds of thank-yous.

  %   \vspace{1\baselineskip}
  %   \noindent
  %   ... continued. 
}

\vfill


{\noindent\footnotesize %
  This work has been using language resources and tools developed and/or stored and/or distributed by the  LINDAT/CLARIN project of the Ministry of Education, Youth and Sports of the Czech Republic (project LM2015071).
}

\cleardoublepage{}
\addcontentsline{toc}{chapter}{Table of Contents}
\tableofcontents % automatically generated

\cleardoublepage{}
\renewcommand{\chapterheadstartvskip}{\vspace*{-10mm}} % chapter spacing
\setstretch{1.2} % line spacing

%
% TEXT START
%
\renewcommand{\thepage}{\arabic{page}}
\setcounter{page}{1}




\sloppy
% \include{01-intro}
% \include{02-method}
% \include{03-conclusions}

\chapter{Introduction}
\label{chap:intro}
% The natural language comes \textit{natural} primarily to humans, to computers less so. 
The key to computers' versatility and efficiency---their ``language''---are data structures: arrays, lists, trees and graphs, tables and databases. As we use the computers to ground our decisions, it is crucial that we undestand and interpret the data stored in these structures. We can scrutinize the data with appropriate tools, provided we have sufficient domain expertise and enough time, but this does not address the core of the problem: that to most people, reading structured data is not \textit{natural} the way the natural language is. Can we, instead, harness the versatility of computers and let them describe the data in our language?

% Eventually, as we try to understand and interpret the data stored in these structures. Do we need to scrutinize these structures ourselves, or can we hope to teach the computers to translate the data into our language?

Since the dawn of computing, people have believed that we can. The first attempts at producing natural language date back to the audacious attempts of \textit{translating} between English and Russian in 1950's \cite{sheridan1955research}. These initial have stirred a lot excitement and lead to belief that \textit{generating} English sentences is a simpler task. Although in 1960's, people slowly began to ponder on its difficulties---\citet{yngve1961random} notes even the first ten sentences of a simple children's book provide \textit{``surprisingly wide linguistic diversity''}, making it hard to assemble appropriate grammar rules---the overall sentiment was that language generation will soon be solved. The seminal work of \citet{winograd1971procedures}, describing in 461 pages the SHRDLU system which manipulate blocks in an imaginary block world according to user instructions, only glosses over presenting the state of the world to the user:
\begin{pquotation}{\citealp[p.384]{winograd1971procedures}}
  [R]esponses can be made as complex and varied as we want, since they are created by the programmer, and the program only repeats them.
\end{pquotation}
In other words: the computers can mechanically repeat whatever we say; what else is there to generating language?

Fast forward to the present, the research world is beaming with excitement again: neural \acp{lm} have a suprising ability of producing the long-sought \textit{complex} and \textit{varied} language \cite{radford2019language,brown2020language}. Similarly to other tasks in \ac{ai}---from object recognition \cite{papert1966summer} to self-driving cars \cite{autonomouscars}---the apparent ease of the task for humans has proven deceptive. It took us 50 years to get there and it only became possible with recent advances in \ac{ai}. To make progress, we had to shift our attention from linguistic theories and rule-based systems, re-defining our systems in terms of data-based approaches and generic learning algorithms.

In the previous decades, the goal of systems for automatic \ac{nlg}---which has later established itself as a standalone scientific discipline, with its journals, conferences, and stable base of researchers \cite{ACLanthologySIGGEN}---was rather pragmatic.
% taking structured data from a particular system  and presenting it to the users in the form they will understand. 
The works in \ac{nlg} were the works of \textit{engineering}: natural language was simply taken as one of the suitable mediums to present the structured data to the users in an understandable form. From weather forecasts \cite{belzAutomaticGenerationWeather2008} to chart captions \cite{demirSummarizingInformationGraphics2012}, the research papers read like \textit{how-to's} for building systems with widely adopted tools. As a result, the \ac{nlg} systems from that time were accurate and reliable, if only a bit too domain-specific and rigid \cite{reiterBuildingAppliedNatural1997,gattSurveyStateArt2018}.

% The essence of these systems was transforming a non-linguistic inputs to linguistic outputs according to a sequence of rule. In some sense, we can therefore say that transforming data to text---what is now specifically called \ac{d2t} generation---was all there was to \ac{nlg}.

With neural models, the  \ac{nlg} as a research field has changed. First, it has gotten more experimental. Neural \acp{lm} opened up new possibilities in building end-to-end systems and solving the long-standing issues with fluency and domain-independence \cite{ferreiraNeuralDatatotextGeneration2019,dusekEvaluatingStateoftheartEndtoEnd2020,sharmaInnovationsNeuralDatatotext2022}. However, working with neural models had turned out to be closer to behavioral sciences than engineering \cite{holtzmanGenerativeModelsComplex2023}. Before researchers learned to properly deal with the paradigm shift, it lead to issues with respect to experimental design and evaluation \cite{gehrmannRepairingCrackedFoundation2022} and was percieved as a step back by some \cite{reiter2020academic}. This shift towards experimental approaches has also created a gap between research and industry; the industry opting for established approaches in the ever-changing research landscape \cite{daleNaturalLanguageGeneration2020,daleNavigatingTextGeneration2023}.


However, this progressive approach turned out to have its merits. The emphasis on open research---where publicly releasing papers, code, and models has become a de-facto standard in recent years---has allowed everybody to stand on the proverbial shoulders of giants. As anybody can build on other's code within minutes since it is made public, the research is accelerating. The convergence towards generic aproaches in \ac{nlp} has also lead to heavy cross-pollination, making ideas for specific tasks applicable to other tasks. As such, \ac{nlg} is helping to advance other areas of \ac{nlp} and contribute to general knowledge on natural language, its production and processing.

And finally, as we gained other ways to generate language than from structured data: summarize and paraphrase other texts, continue incomplete text segments, generate answers to questions, or describe images and videos \cite{Dong2021ASO}, the field with the original aim of generating texts from structured data has gradually adopted the---perhaps more apt---name of \textit{\ac{d2t} generation}.
% As one of the \ac{nlg} subfields, it continues to try to solve some of the long-stading issues with language fluency or flexibility of the systems.






\section{Motivation}
\label{sec:rq}

The thesi aims to reconcile the gap outlined in the introduction: taking experimental approaches and turning them into reliable and accurate systems. The thesis pivots around neural models which offer new opportunities in making \ac{d2t} generation

Specifically, we aim to answer the following research questions:

\begin{enumerate}
  \item \textbf{How to process the structured data with neural \acp{lm}?} The \acp{lm} were processed on text modeling, while the
  \item \textbf{How to make a system built on neural \acp{lm} controllable?}
  \item \textbf{How to evaluate the outputs of \ac{d2t} generation systems?}
  \item \textbf{How do the \ac{d2t} generation systems built on neural \acp{lm} behave?}
\end{enumerate}



\section{Main Contributions}
\label{sec:contributions}


To achieve that, we will encounter several recurring themes:
\begin{enumerate}
  \item \textit{transformation} of structured data into format processable by neural models,
  \item \textit{unification} of data formats for making the systems domain-independent,
  \item \textit{modularization} of the systems for making the systems controllable.
\end{enumerate}



\section{Thesis Overview}
\label{sec:overview}


The thesis inevitably reflects the shifts in \ac{nlp} research between 2020 and 2024: from early attempts at generating fluent language with neural \acp{lm}, towards dealing with issues of domain generalization and semantic accuracy with larger models.
Over the course of
On the way, we touch various facets of \ac{d2t} generation: improving \ac{d2t} generation itself (\autoref{chap:low-res}), evaluating generated texts (\autoref{chap:evaluation}), processing and visualizing data (\autoref{chap:tabgenie}), and investigating system behavior (\autoref{chap:investigating}).

\chapter{Background}
\label{chap:background}
\section{Neural Language Models}
\label{sec:lms}
\subsection{Neural Networks}
\label{sec:nns}
\subsection{Transformer Architecture}
\label{sec:transformer}
\subsection{Pretrained Language Models}
\label{sec:plms}
\subsection{Large Language Models}
\label{sec:llms}
\section{Data-to-Text Generation}
\label{sec:d2t}
\subsection{Rule-based Approaches}
\label{sec:rule-d2t}
\subsection{Neural Approaches}
\label{sec:neural-d2t}
\subsection{Datasets}
\label{sec:datasets}
\subsection{Evaluation Metrics}
\label{sec:evaluation}

\chapter{Low-Resource Data-to-Text Generation}
\label{chap:low-res}
\section{Motivation}
\label{sec:low-res-mot}
\section{Finetuning LMs}
\label{sec:finetuning}
\subsection{WebNLG+ Shared Task}
\label{sec:webnlgp}
\subsection{Our Submission}
\label{sec:mbart}
\section{Iterative Template Fusion with Text-Editing LMs}
\label{sec:iterative}
\subsection{Text-Editing LMs}
\label{sec:text-editing}
\subsection{Experiments}
\label{sec:text-editing-exp}
\section{Pipelined Text-Based Operations with Pretrained LMs}
\label{sec:pipeline}
\subsection{Pipeline Operations}
\label{sec:pipeline-ops}
\subsection{Experiments}
\label{sec:pipeline-exp}

\chapter{Evaluating Generated Text}
\label{chap:evaluation}
\section{Motivation}
\label{sec:evalution-mot}
\section{Evaluating Semantic Accuracy}
\label{sec:sem-acc}
\subsection{Experiments}
\label{sec:sem-acc-exp}
\section{Token-Level Error Detection}
\label{sec:eval-token}
\subsection{Shared Task}
\label{sec:eval-st}
\subsection{Our Submission}
\label{sec:eval-ours}

\chapter{Data Processing and Visualization}
\label{chap:tabgenie}
\section{Motivation}
\label{sec:data-mot}
\section{TabGenie Toolkit}
\label{sec:tabgenie}
\subsection{Data Processing}
\label{sec:tabgenie-data}
\subsection{Web Interface}
\label{sec:tabgenie-web}
\subsection{Programming Interface}
\label{sec:tabgenie-cli}


\chapter{Investigating Model Capabilities}
\label{chap:investigating}
\section{Motivation}
\label{sec:investigating-mot}
\section{Describing Triples in Knowledge Graphs}
\label{sec:describing}
\subsection{Knowledge Graphs}
\label{sec:kgs}
\subsection{Rel2Text Dataset}
\label{sec:rel2text}
\subsection{Experiments}
\label{sec:rel2text-exp}
\section{Prompting Open LLMs}
\label{sec:prompting}
\subsection{\textsc{Quintd} Toolkit}
\label{sec:quintd}
\subsection{Experiments}
\label{sec:quintd-exp}




\chapter{Conclusions}
\label{chap:conclusions}


%
% TEXT END
%

\renewcommand{\chapterheadstartvskip}{\vspace*{0mm}} % chapter spacing

\cleardoublepage{}
\bibliographystyle{csplainnat}
\addcontentsline{toc}{chapter}{Bibliography}
{\small \bibliography{references}}

\cleardoublepage{}
\addcontentsline{toc}{chapter}{List of Abbreviations}
\renewcommand*{\acronymname}{List of Abbreviations}
\printglossary[type=\acronymtype,style=index]

\addcontentsline{toc}{chapter}{List of Tables}
{\small \listoftables\par}

\addcontentsline{toc}{chapter}{List of Figures}
{\small \listoffigures\par}

\cleardoublepage{}
\addcontentsline{toc}{chapter}{List of Publications}
\include{list_of_publications}

\end{document}
