
\chapter{Conclusions}
\label{chap:conclusions}

In this thesis, we set out to explore how to employ neural \acp{lm} in \ac{d2t} generation systems. The issues we were trying to solve---semantic inaccuracies in the generated texts, lack of automatic evaluation metrics, heterogeneous input data format, and unknown scope of \acp{lm} abilities---are as pressing as ever, despite the increase in \ac{lm} capabilities over the past few years. That is not to say that there has not been any progress: the opposite is evidenced by numerous works introduced in \autoref{chap:background} (and others we did not get a chance to mention).


Hopefully, we also contributed to the progress with our findings, including:
\begin{itemize}
    \item Our approaches for efficient application of \acp{plm} in \ac{d2t} generation systems (\autoref{chap:low-res}),
    \item Our \ac{plm}-based automatic metrics for evaluating semantic accuracy of generated texts (\autoref{chap:evaluation}),
    \item Our toolkit for unified processing and visualization of existing \ac{d2t} generation benchmarks (\autoref{chap:tabgenie}),
    \item Our custom benchmarks for analyzing generalization abilities of \acp{lm} to novel domains and formats (\autoref{chap:investigating}).
\end{itemize}

At this point, we should take a step back and revisit the basic questions: Which problems in \ac{d2t} generation are the most important? Do we want to continue integrating \aclp{lm} in \ac{d2t} generation systems? And is there value in developing specialized approaches, or are all the problems of the field going to be solved by using ever larger models?

A good starting point to answer these questions is realizing that \aclp{lm} are here to stay. The ongoing proliferation of \acp{lm} in \acl{nlp} \cite{min2023recent,zhao2023survey,naveed2024comprehensive} makes it hard to imagine a subfield that would be left intact by their impact. There is a solid reason for that: with \acp{lm}, certain things unimaginable during previous decades---such as fine-grained steering of a system using natural language instructions---are now becoming possible. We can expect that \ac{d2t} \emph{without \acp{lm}} would, to put it bluntly, soon start to feel awkward. People are already becoming used to consuming fluent texts and seamlessly interacting with language generation systems, aspects that are hard to replace with non-\ac{lm} systems. As we stated in the introduction, \ac{d2t} generation is primarily about simplifying interactions of end users, so these aspects cannot be neglected if \ac{d2t} research is to stay relevant.

At the same time, the users are (hopefully, along with us researchers) becoming aware of the limitations of \ac{lm}-based systems. Even the most powerful \acp{lm} nowadays cannot reliably perform symbolic operations \cite{qian2023limitations}, understand reflexivity of relations \cite{berglund2024the}, or recognize unplausible requests \cite{yin2023large}. All of these issues are tied to \ac{d2t} generation: for example, recognizing invalid inputs is crucial for correct verbalization of data in case the data is ambiguous, as we discussed in \autoref{sec:rel2text}. It is reasonable to expect that these issues will not be fully solved with further scaling or minor architectural improvements. We therefore need to tread carefully when integrating \acp{lm} in \ac{d2t} generation systems: a system relying too heavily on \acp{lm} may not be ever able to guarantee outputs accurate enough for day-to-day usage, let alone for sensitive applications.

It would be, however, counter-productive to dismiss \acp{lm} by likening them to a ``black box'', picking on their unpredictable behavior. It is better to acknowledge that even the \emph{black boxes} are still \emph{boxes}: components that can be embedded in a larger system. As we repeatedly showed throughout the thesis, such a component can be helpful when used wisely. We can, for example, over-generate \ac{lm} outputs and select only the relevant ones (\autoref{sec:iterative}) or train the \ac{lm} in a way that its outputs are more predictable (\autoref{sec:pipeline}). We can also note the tasks on which \acp{lm} achieve state-of-the-art performance, such as natural language inference (\autoref{sec:sem-acc}) or text classification (\autoref{sec:tok-eval}), and build our system around these tasks.

Looking at recent developments, we only scratched the surface of what is possible. Even now, we can go beyond lexicalization and surface realization by connecting \acp{lm} to tools such as a Python interpreter or an SQL engine, enabling \acp{lm} to perform content selection as well \cite{cao-etal-2023-api,jiang-etal-2023-structgpt,gemmell2023generate}. We can imagine that by combining code execution with approaches such as chain-of-thought prompting and its advanced variants \cite{weiChainThoughtPrompting2022,chu2023survey}, the systems will be able to automatically perform logical operations over the data to derive interesting insights \cite{zhao-etal-2023-qtsumm,chenLogicalNaturalLanguage2020,chenLogic2TextHighFidelityNatural2020}. Soon, we may think of literal data transcription or shallow data summarization the way we think about, for example, word-for-word translation: as an approach that is too basic to even consider. \ac{lm}-powered systems could thus get us closer to presenting useful insights from large-scale structured data, the ultimate purpose of \ac{d2t} generation.

As the systems get better at handling multiple tasks in a unified way, the role of individual tasks could be somehow downplayed. A single \ac{lm}-based component could jointly tackle all the tasks that are currently thought of as stand-alone: natural language understanding, text-to-SQL, data mining, question answering, or data-to-text generation \cite{schopf-etal-2023-exploring,chen2024multi}. Rather than in the tasks themselves, the researchers would then specialize in auxiliary tools used on top of the models such as the approaches for steering the generation process, output quality assurance, or personalization.


Evaluating future systems will admittedly get more difficult. The first step we need to focus on is making the current evaluation practices comparable and relevant, an issue which is already perceived as urgent by the community \cite{gehrmannRepairingCrackedFoundation2022,van_miltenburg_barriers_2023}.  As we discussed in \autoref{sec:quintd}, this means, e.g., caring about data contamination and re-examining our approach to research benchmarks. In the long run, we should also focus on the ecological validity of the systems we are developing. To achieve that, we should focus more on extrinsic evaluation, i.e., evaluating the system as a whole instead of its individual components (\autoref{sec:evaluation}). These measures are harder to iterate on but give us a better picture of the real-world impact of the systems we are building.

A final recommendation, that perhaps should have come a bit sooner: \emph{look in the data and try to perform the task yourselves first}. As much as we introduced the data as the language of computers, its content and structure can always be traced down to the human-made inputs. Our experiments---such as the ones in \Cref{sec:tabgenie,sec:rel2text}---made it clear that the inputs are often messy, incomplete, and illegible. In these cases, trying to present the data in understandable form is equivalent to translating gibberish from one language to another. Until the systems can detect these cases automatically, we will need to do the legwork and keep on learning the language of the data we are producing.