
\chapter{Conclusions}
\label{chap:conclusions}

In this thesis, we set out to explore how to employ neural \acp{lm} in \ac{d2t} generation systems. The issues we tackled---semantic inaccuracies in the generated texts, lack of suitable automatic evaluation metrics, heterogeneous input data format, and unknown scope of \acp{lm} abilities---are as pressing as ever, in spite of the rapid increase in \ac{lm} capabilities over the past few years. That is not to say that there has not been any progress: the opposite is evidenced by numerous works introduced in \autoref{chap:background}, along with many others.


Hopefully, we also contributed to the progress with our findings, including:
\begin{itemize}
    \item Our approaches for efficient employment of \acp{plm} in \ac{d2t} generation systems (\autoref{chap:low-res}),
    \item Our \acp{plm}-based automatic metrics for evaluating semantic accuracy of generated texts (\autoref{chap:evaluation}),
    \item Our toolkit for unified processing and visualization of existing \ac{d2t} generation benchmarks (\autoref{chap:tabgenie}),
    \item Our custom benchmarks for analyzing generalization abilities of neural \acp{lm} to novel domains and formats (\autoref{chap:investigating}).
\end{itemize}

We feel that now is a good time to pause and ponder. Where does \ac{d2t} generation stand in the current research landscape and how do \acp{lm} relate to it? Is there any value in developing \ac{lm}-based \ac{d2t} generation systems? Are all the problems of the field going to be solved (or made irrelevant) by using ever larger models?

% Or will the problems perhaps not be even relevant anymore?

First, it is clear that \ac{lm}-based systems are here to stay. As neural \acp{lm} proliferate in \ac{nlp}, it is hard to imagine that an \ac{nlp} subfield would be left intact by their impact. And there is a good reason for that: with \acp{lm}, certain things that were unimaginable during previous decades of research (such as fine-grained steering of a system using free-form natural language instructions) are now becoming possible. To put it bluntly, \ac{d2t} \emph{without \acp{lm}} would start to feel awkward quite soon. People are already becoming used to consuming fluent texts and seamlessly interacting with language generation systems, aspects that are hard to replace by any non-\ac{lm} approach. And, as we stated in the introduction, \ac{d2t} generation is primarily simplifying interactions of end users with large amounts of structured data, so these aspects cannot be neglected.

At the same time, the users (hopefully, along with us researchers) are becoming aware of limitations of \ac{lm}-based systems. Even the most powerful \acp{lm} nowadays cannot reliably count words in a sentence, multiply larger numbers, or recognize unplausible requests. Although the progress is ongoing, it is not very plausible that these issues will be satisfactorily solved with further scaling or minor architectural improvements. These issues are more pronounced in \ac{d2t} generation, with its focus on deriving conclusions from structured information and producing outputs which are accurate and relevant. In the very least, this tells us that we need to tread carefully when integrating \acp{lm} in current \ac{d2t} generation systems.

It would be, however, counter-productive to dismiss \acp{lm} by likening them to a ``black box'': as in ``an artifact with unpredicatable behavior''. It is better to acknowledge that even the \emph{black boxes} are still \emph{boxes}: components that can be embedded in a larger system. As we have repeatedly showed throughout the thesis, such a component can be helpful when used wisely. We can, for example, filter the \ac{lm} outputs to select only the ones which are relevant (\autoref{sec:iterative}), train the \ac{lm} in a way that its outputs are more predictable (\autoref{sec:pipeline}), or simply make use of a state-of-the-art performance of an \ac{lm} on a certain task (\autoref{sec:sem-acc}).

Moreover, the recent developments suggest that we only scratched the surface of what is possible. The general idea of connecting \acp{lm} to data preprocessors, filters, re-rankers, Python interpreters and other tools seems to be a surefire way towards building more powerful systems for interacting with structured data. And along with that come other ideas from the general research of \acp{llm} for \ac{nlp}: chain-of-thought prompting, retrieval-augmented generation, ecosystem of specialized \acp{lm}. Taken all together, there currently seems to be no bound on improvements.

It is interesting to notice, that in these systems, \ac{d2t} generation itself is dissolving and becoming more transparent to both users and researchers. We can envisage powerful data analytics tools combining components for data mining, natural language understanding, text-to-SQL, table question answering, many of which may be tackled by the same component.

Evaluating such systems will, admittedly, get more difficult. Iterating over improving \acs{bleu} is not a thing driving a research forward even nowadays. Recent years have witnessed the decline of classical evaluation metrics; future years will perhaps witness the decline of classical benchmarks. Similarly as we , we will need to focus on extrinsic evaluation and ecological validity.
