
\chapter{Conclusions}
\label{chap:conclusions}

In this thesis, we set out to explore how to employ neural \acp{lm} in \ac{d2t} generation systems. The issues we were trying to solve---semantic inaccuracies in the generated texts, lack of automatic evaluation metrics, heterogeneous input data format, and unknown scope of \acp{lm} abilities---are as pressing as ever, despite the increase in \ac{lm} capabilities over the past few years. That is not to say that there has not been any progress: the opposite is evidenced by numerous works introduced in \autoref{chap:background} (along with many others we did not get a chance to mention).


Hopefully, we also contributed to the progress with our findings, including:
\begin{itemize}
    \item Our approaches for efficient application of \acp{plm} in \ac{d2t} generation systems (\autoref{chap:low-res}),
    \item Our \acp{plm}-based automatic metrics for evaluating semantic accuracy of generated texts (\autoref{chap:evaluation}),
    \item Our toolkit for unified processing and visualization of existing \ac{d2t} generation benchmarks (\autoref{chap:tabgenie}),
    \item Our custom benchmarks for analyzing generalization abilities of neural \acp{lm} to novel domains and formats (\autoref{chap:investigating}).
\end{itemize}

At this point, we should take a step back and revisit the basic questions. Are the problems we are solving in \ac{d2t} generation still relevant? Do we want to continue integrating \acp{lm} in \ac{d2t} generation systems? And is there value in developing specialized approaches, or are all the problems of the field going to be solved by using ever larger models?

A good starting point to answer these questions is realizing that \acp{lm} are here to stay. The ongoing proliferation of \acp{lm} in \ac{nlp} makes it hard to imagine an \ac{nlp} subfield left intact by impact of \acp{lm}. And there is a reason for that: with \acp{lm}, certain things unimaginable during previous decades---such as fine-grained steering of a system using natural language instructions---are now becoming possible. We can expect that \ac{d2t} \emph{without \acp{lm}} would soon start to feel awkward, to put it bluntly. People are already becoming used to consuming fluent texts and seamlessly interacting with language generation systems, aspects that are hard to replace with non-\ac{lm} systems. As we stated the introduction, \ac{d2t} generation is primarily about simplifying interactions of end users with large amounts of structured data, so these aspects cannot be neglected if \ac{d2t} research is to stay relevant.

At the same time, the users (hopefully, along with us researchers) are becoming aware of limitations of \ac{lm}-based systems. Even the most powerful \acp{lm} nowadays cannot reliably count words in a sentence, multiply larger numbers, or recognize unplausible requests. Most probably, these issues will not be fully solved with further scaling or minor architectural improvements. We therefore need to tread carefully when integrating \acp{lm} in \ac{d2t} generation systems: a system relying too heavily on \acp{lm} may not be ever able to guarantee outputs accurate enough for day-to-day usage, let alone for sensitive applications.

It would be, however, counter-productive to dismiss \acp{lm} by likening them to a ``black box'', picking on their unpredictable behavior. It is better to acknowledge that even the \emph{black boxes} are still \emph{boxes}: components that can be embedded in a larger system. As we have repeatedly showed throughout the thesis, such a component can be helpful when used wisely. We can, for example, over-generate \ac{lm} outputs and select only the relevant ones (\autoref{sec:iterative}), train the \ac{lm} in a way that its outputs are more predictable (\autoref{sec:pipeline}), or simply make use of the state-of-the-art performance of \acp{lm} on tasks such as natural language inference or text classification (\Cref{sec:sem-acc,sec:tok-eval}).

Looking at recent developments, we only scratched the surface of what is possible. Even now, we can, for example, go beyond lexicalization and surface realization by connecting \acp{lm} to tools such as a Python interpreter or an SQL engine, enabling \acp{lm} to perform content selection as well. We can imagine that by combining code execution with approaches such as chain-of-thought prompting, the systems will be able to perform logical operations over the data to derive interesting insights. Soon, we may think of literal data transcription or shallow data summarization the way same we think about, for example, word-for-word translation: as an approach that is too basic to even consider. \ac{lm}-powered systems could thus get us closer to the ultimate purpose of \ac{d2t} generation: presenting useful insights from large-scale structured data.

We can also speculate that in these interconnected systems, the role of \ac{d2t} generation as a stand-alone task will be somehow downplayed. Researchers will be specializing rather in tools for powering the tasks than in the tasks themselves, tackling jointly what is now called natural language understanding, text-to-SQL, data mining, table question answering, or data-to-text generation.

Evaluating such systems will, admittedly, get more difficult. Iterating over improving \acs{bleu} is not a thing driving a research forward even nowadays. Recent years have witnessed the decline of classical evaluation metrics; future years will perhaps witness the decline of classical benchmarks. Similarly as we , we will need to focus on extrinsic evaluation and ecological validity.
