While traditional systems for data-to-text generation can guarantee high levels of semantic accuracy, their fluency and adaptability to new domains remains limited. We explore how to improve these aspects with neural components.
% systems for generating and evaluating text based on structured data, using pretrained language models as basic building blocks. For text generation, we propose
First, we introduce data-efficient approaches for data-to-text generation that can maintain high levels of semantic accuracy. To assess the accuracy, we introduce two automatic metrics based on pretrained language models. Next, we turn to datasets: we introduce a framework unifying data-to-text generation datasets in common format which enables efficient data processing and visualization. Using custom datasets, we investigate the capability of pretrained language models to verbalize unseen relations in knowledge graphs and the capability of large language models to generate texts from structured data in standard formats. We conclude that while data-to-text generation with neural language models remains a delicate endeavor, neural components can be used to improve text fluency and extend the scope of data-to-text generation beyond the limited set of formats and domains used until now. We also emphasize the need for rigorous evaluation, for which new evaluation metrics and ways of bechmarking are needed.
% We also emphasize the potential of neural language models for automatic text evaluation, and suggest that new ways of benchmarking are needed.