Current systems for data-to-text generation can guarantee high levels of semantic accuracy, but their fluency and adaptability to new domains remains limited. In this thesis, we explore how to improve these aspects with neural components. First, we introduce data-efficient approaches for data-to-text generation that can maintain high levels of semantic accuracy, and introduce two automatic metrics for assessing the semantic accuracy. Next, we turn to data-to-text generation datasets: we unify the datasets in a common format to efficiently visualize the data and process them with neural models. We also collect custom datasets to investigate the capability of neural language models to verbalize unseen relations in knowledge graphs and to generate accurate texts from standard data formats. We conclude that while data-to-text generation with neural language models remains a delicate endeavor, neural components can be used to improve text fluency. The models also help to extend the scope of data-to-text generation beyond the limited set of domains and data input formats. However, we emphasize the need for rigorous evaluation with suitable metrics and new ways of benchmarking.