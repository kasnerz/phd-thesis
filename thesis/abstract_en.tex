Current systems for data-to-text generation can guarantee high levels of semantic accuracy, but their fluency and adaptability to new domains remains limited. In this thesis, we explore how to improve these aspects with neural components. First, we introduce data-efficient approaches for data-to-text generation that can maintain high levels of semantic accuracy, using pretrained language models as building blocks. We also introduce two model-based automatic evaluation metrics for assessing the semantic accuracy. Turning to data-to-text generation datasets, we unify existing datasets in a common format, enabling efficient visualization and processing. Using custom datasets, we investigate generalization capabilities of neural language models: the ability to verbalize unseen relations in knowledge graphs and the ability to generate accurate texts from standard data formats. We conclude that while data-to-text generation with neural language models remains a delicate endeavor, in a constrained setup, neural components are the preferred approach for improving text fluency. Neural language models also make the systems more easily adaptable to new domains and input formats. For the future research, we emphasize the need for proper benchmarking with suitable evaluation metrics on real-world use-cases.