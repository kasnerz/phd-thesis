% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
\label{chap:intro}
Producing \emph{natural language} comes \emph{natural} primarily to us, humans.
The key to computers' versatility and efficiency---their ``language''---are data structures: arrays, lists, trees and graphs, tables and databases.
We can scrutinize the data with appropriate tools---provided sufficient domain expertise and enough time---but this does not address the core of the problem: that to most people, reading structured data is like trying to decipher a foreign language. As the volume of data in our world grows, it is tempting to turn the question on its head: Can we instead teach the computers to describe the structured data in our, natural language?


This question has been addressed since the dawn of computing. The first attempts at producing natural language with a computer date back to the audacious attempts of \emph{translating} between English and Russian in 1950's \cite{sheridan1955research} which stirred a lot of excitement, and lead to a belief that \emph{generating} English sentences with a set of rules is a simpler task. Although in 1960's, people slowly began to ponder on its difficulties---\citet{yngve1961random} notes even the first ten sentences of a children's book provide \emph{``surprisingly wide linguistic diversity''} for assembling appropriate grammar rules---the overall sentiment was that language generation will soon be solved. The seminal work of \citet{winograd1971procedures}, describing in 461 pages the SHRDLU system which manipulates blocks in an imaginary block world according to user instructions, only glosses over presenting the state of the world to the user:
\begin{pquotation}{\citealp[p.384]{winograd1971procedures}}
    [R]esponses can be made as complex and varied as we want, since they are created by the programmer, and the program only repeats them.
\end{pquotation}
In other words: If we can make the computers mechanically repeat whatever we say, what else is there to generating language?

Fast forward to the present, the research world is beaming with excitement again: neural \acp{lm} have a suprising ability of producing the long-sought \emph{complex} and \emph{varied} language \cite{radford2019language,brown2020language}. Similarly to other tasks in \ac{ai}---from object recognition \cite{papert1966summer} to self-driving cars \cite{autonomouscars}---the apparent ease of the task for humans has proven deceptive. In the end, ot took us 50 years to build tools for generating fluent language. To make progress, we had to shift our attention from linguistic theories and rule-based systems, re-defining our systems in terms of data-based approaches and generic learning algorithms.

In the previous decades, in which language was generated using rules and grammars, the goal of systems for automatic \ac{nlg}---which has meanwhile established itself as a standalone scientific discipline, with its journals, conferences, and stable base of researchers \cite{ACLanthologySIGGEN}---was rather pragmatic.
% taking structured data from a particular system  and presenting it to the users in the form they will understand. 
The works in \ac{nlg} were the works of \emph{engineering}: natural language was simply taken as one of the suitable mediums to present the structured data to the users in an understandable form. From chart captioning systems \cite{mittalDescribingComplexCharts1998} and graph descriptors \cite{sunDomainIndependentSentence2006}, to weather forecast systems \cite{belzAutomaticGenerationWeather2008} and healthcare report generators \cite{portetAutomaticGenerationTextual2009}, the research papers read like \emph{how-to's} for building robust systems with widely adopted tools. As a result, the \ac{nlg} systems from that time were accurate and reliable, if only a bit too domain-specific and rigid \cite{reiterBuildingAppliedNatural1997,gattSurveyStateArt2018}.

% The essence of these systems was transforming a non-linguistic inputs to linguistic outputs according to a sequence of rule. In some sense, we can therefore say that transforming data to text---what is now specifically called \ac{d2t} generation---was all there was to \ac{nlg}.

With neural models, \ac{nlp} as a research field (along with \ac{nlg} as one of its subfields) has changed. Most notably, it has become more experimental. Although neural \acp{lm} opened up fascinating possibilities in building end-to-end systems and solving the long-standing issues with fluency and domain-independence \cite{ferreiraNeuralDatatotextGeneration2019,dusekEvaluatingStateoftheartEndtoEnd2020,sharmaInnovationsNeuralDatatotext2022}, working with neural models turned out to be closer to behavioral sciences than engineering \cite{holtzmanGenerativeModelsComplex2023}. As the researchers began adapting to the change in the paradigm, the issues with respect to experimental design and evaluation came to surface \cite{gehrmannRepairingCrackedFoundation2022} and the whole change may have been percieved as a step back \cite{reiter2020academic}. The shift towards experimental approaches has also created a gap between research and industry; the industry opting for established approaches meeting industrial standards
% in the ever-changing research landscape
\cite{daleNaturalLanguageGeneration2020,daleNavigatingTextGeneration2023}.


Nevertheless, the progressive approach adopted by \ac{nlp} over the past few years turned out to have its merits. The general emphasis on open research, inherited from the \ac{ml} field---where publicly releasing papers, code, and models has become commonplace---has allowed everybody to stand on the proverbial shoulders of giants. As people can build on others' code within minutes since its publication, the research is accelerating and gathering more observations. The convergence towards generic aproaches has also lead to heavy cross-pollination of ideas, i.e., making ideas for specific tasks applicable to other tasks. As such, \ac{nlg} is helping to advance other areas of \ac{nlp} and contribute to general knowledge on natural language, its production and processing.

Finally, as we gained other ways to generate language than from structured data---summarize and paraphrase texts, continue text segments, generate stories and answers to questions, or describe images and videos---% \cite{Dong2021ASO}
the original field dealing with generating descriptions of structured data has gradually adopted the---perhaps more apt---name of \emph{\ac{d2t} generation}.

This thesis is a story about how \acl{d2t} generation and neural \aclp{lm} came together, or at least are seeking common ground. On the way, it touches various facets of \ac{d2t} generation, from improving the generation in low-resource settings (\autoref{chap:low-res}), evaluating generated texts (\autoref{chap:evaluation}), processing and visualizing data (\autoref{chap:tabgenie}), to interpreting system behavior (\autoref{chap:investigating}). It tries to make the point that the adoption of neural approaches in \ac{d2t}---although it has not been exactly smooth (and it would be a stretch to call it a \emph{symbiosis})---is driven by the promise that it can help us to solve some long-stading issues. The thesis also inevitably reflects the shifts in \ac{nlp} between 2020 and 2024: from early (sometimes a bit desperate) attempts at generating fluent language with small pretrained \acp{lm}, all the way up to dealing with the hype surrounding the \acp{llm}.  Far from being a complete survey, the thesis is rather explorative, but can hopefully offer both pointers to newcomers in the field along with a handful of unorthodox ideas.










\section{Motivation}
\label{sec:rq}

The main goal of the thesis is to close the gap outlined in the introduction: turning experimental approaches into reliable and accurate \ac{d2t} generation systems. For the premise, we take neural \acp{lm}\footnote{For brevity, we will use ``\acp{lm}'' to denote ``neural \acp{lm}'' throughout the work, unless stated otherwise.} as a mean of producing fluent and natural-sounding text. However, instead of considering \acp{lm} as a hammer and everything as a nail, we carefully study how to integrate \acp{lm} in \ac{d2t} systems while adhering to all the strict demands on fluency, controllability, and semantic accuracy of the output.

The auxiliary goal of the thesis is then to \textit{understand}: understand the data we are dealing with, the outputs we can reasonably expect, and the behavior of neural-based systems in certain conditions. The approaches presented for \ac{d2t} generation can often be generalized to other subfield, even though \ac{d2t} generation has several specifics which makes it a good subject for this kind of study: its low-resourceness (due to which---thankfully---there are questions that cannot be answered with ``\emph{scaling}''), the tension between established rule-based approaches and the new-coming neural approaches, and, admittedly, the fact that it is the field that we have gathered the most expertise in.

To make these goals more tangible, the thesis addresses the following research questions:

\begin{description}
    \item[RQ1] \textbf{In which scenarios are \acp{lm} useful for \ac{d2t} generation?} For starters, it is crucial to identify the strong sides of \acp{lm} and get an intuition of where the models can make the most impact. How far can we get with \ac{lm}-only baselines? Are there outcome that we can get with \ac{lm} which are better than any previous approaches? At the same time, it is essential to understand their limitations, and be able to employ the \acp{lm} only where it is actually needed.
    \item[RQ2] \textbf{How to efficiently process the structured data with \acp{lm}?} With any kind of structured data, we need to deal with the fact that \acp{lm} were pre-trained on modeling plain text, while the data have inner structure. In order to efficiently leverage the knowledge in \acp{lm}---especially in low-resource settings---we need to find the way to transform the data into a suitable input format while keeping the structure of the data (along with other information) intact.
    \item[RQ3] \textbf{How to make \acp{lm}-based systems more controllable?} Any neural component introduced in the \ac{d2t} generation system will raise issues with controllability. The question is if we can minimize these issues, for example by building systems out of smaller and simpler components, training the models for more predictable tasks, or producing intermediate outputs which can be manually examined.
    \item[RQ4] \textbf{How to evaluate the outputs of \ac{d2t} generation systems?} Evaluating generated text is hard, and it gets even harder as the quality of the texts starts to approach human level. Since human evaluation is costly and time-consuming, we study how to build automatic metrics for researchers and the system developer, focusing on the most pressing issue in \ac{d2t} generation: the semantic accuracy of the generated texts.
    \item[RQ5] \textbf{How do the neural \ac{d2t} generation systems behave?} Undestanding the abilities and limitations of the systems we are building is crucial both for researchers and for potentional practicioners in the field. We are interested in measuring the abilities of \ac{lm}-based systems and pointing out their weak spots.
\end{description}



\section{Main Contributions}
\label{sec:contributions}


Our main contributions, corresponding numerically to the research questions outlined above, are:
\begin{enumerate}
    \item We show that with a very \textbf{simple \ac{lm}-based finetuned baseline}, we can achieve strong results on a shared task of generating texts from a knowledge graph (\autoref{sec:finetuning}). We also point out the advantages and limitations of open \acp{llm} on \ac{d2t} generation in zero-shot settings (\autoref{sec:prompting}).
    \item We show how to \textbf{transform the data to intermediate text-like input} suitable for \acp{lm} using hand-crafted or automatically extracted templates (\Cref{sec:iterative,sec:pipeline,sec:sem-acc}), rule-based \ac{nlg} methods (\autoref{sec:eval-token}), and specialized \acp{lm} (\autoref{sec:describing}). We show that these methods can serve as a basis both for competitive neural-based \ac{d2t} generation systems and for novel \ac{lm}-based evaluation metrics.
    \item We show how we can limit \acp{lm} to the task of \textit{improving text fluency}, and use these \acp{lm} for building \textbf{more controllable \ac{d2t} generation systems} with an iterative approach (\autoref{sec:iterative}) and modular architecture (\autoref{sec:pipeline}).
    \item We develop \textbf{\ac{lm}-based automatic metrics} for evaluating outputs of \ac{d2t} generation systems on the level of data item mentions (\autoref{sec:sem-acc}) and output tokens (\autoref{sec:eval-token}), showing strong results regarding correlation with human judgement in comparison with other available metrics.
    \item We build a \textbf{tool for visualizing} the structured data and model outputs and show how we can unify the format of mutliple \ac{d2t} generation dataset for easier processing (\autoref{sec:tabgenie}). We also \textbf{study the behaviors of open \acp{llm}} across multiple \ac{d2t} tasks, data formats, and domains, evaluate their semantic accuracy, and provide recommendations for future research (\Cref{sec:prompting,sec:describing}).
\end{enumerate}



\section{Thesis Overview}
\label{sec:overview}
% \subsection*{\autoref{chap:low-res}}

% \begin{itemize}
%     \item Finetuning LMs: Train Hard, Finetune Easy: Multilingual Denoising for RDF-to-Text Generation \cite{kasnerTrainHardFinetune2020}
% \end{itemize}

\begin{table*}[ht]
    \small
    \begin{tabular}{p{4cm}p{7.5cm}p{1cm}}
        \toprule
        \textbf{Publication}                             & \textbf{Author contribution} & \textbf{Sec.}         \\ \midrule
        \multicolumn{3}{l}{\textbf{\autoref{chap:low-res}: Low-Resource Data-to-Text Generation}}               \\
        \citet{kasnerTrainHardFinetune2020}              & TODO                         & §\ref{sec:finetuning} \\
        \citet{kasnerDatatoTextGenerationIterative2020}  & TODO                         & §\ref{sec:iterative}  \\
        \citet{kasner2022neural}                         & TODO                         & §\ref{sec:pipeline}   \\ \cdashlinelr{1-3}
        \multicolumn{3}{l}{\textbf{\autoref{chap:evaluation}: Evaluating Generated Text}}                       \\
        \citet{dusekEvaluatingSemanticAccuracy2020}      & TODO                         & §\ref{sec:sem-acc}    \\
        \citet{kasnerTextinContextTokenLevelError2021}   & TODO                         & §\ref{sec:eval-token} \\ \cdashlinelr{1-3}
        \multicolumn{3}{l}{\textbf{\autoref{chap:tabgenie}: Data Processing and Visualization}}                 \\
        \citet{kasnerTabGenieToolkitTabletoText2023}     & TODO                         & §\ref{sec:tabgenie}   \\ \cdashlinelr{1-3}
        \multicolumn{3}{l}{\textbf{\autoref{chap:investigating}: Investigating Model Capabilities}}             \\
        \citet{kasnerMindLabelsDescribing2022}           & TODO                         & §\ref{sec:describing} \\
        \citet{kasnerReferenceBasedMetricsAnalyzing2024} & TODO                         & §\ref{sec:prompting}  \\\bottomrule
    \end{tabular}
\end{table*}